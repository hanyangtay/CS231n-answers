{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "print 'dev data shape: ', X_dev.shape\n",
    "print 'dev labels shape: ', y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.363319\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.677276 analytic: 1.677276, relative error: 6.478520e-09\n",
      "numerical: 2.111173 analytic: 2.111173, relative error: 1.532405e-08\n",
      "numerical: 0.665878 analytic: 0.665878, relative error: 4.169243e-08\n",
      "numerical: -0.981633 analytic: -0.981633, relative error: 1.069003e-08\n",
      "numerical: -0.220953 analytic: -0.220953, relative error: 1.001361e-07\n",
      "numerical: -3.732653 analytic: -3.732654, relative error: 1.304863e-08\n",
      "numerical: -0.870953 analytic: -0.870953, relative error: 1.025893e-08\n",
      "numerical: 0.833423 analytic: 0.833423, relative error: 8.965944e-08\n",
      "numerical: -3.103738 analytic: -3.103738, relative error: 5.204987e-09\n",
      "numerical: 1.090576 analytic: 1.090576, relative error: 8.439202e-09\n",
      "numerical: 1.954812 analytic: 1.954812, relative error: 1.568653e-08\n",
      "numerical: 0.345197 analytic: 0.345197, relative error: 1.857520e-08\n",
      "numerical: -1.036041 analytic: -1.036041, relative error: 1.585448e-09\n",
      "numerical: -1.363838 analytic: -1.363838, relative error: 3.548173e-09\n",
      "numerical: 0.412956 analytic: 0.412956, relative error: 1.274951e-07\n",
      "numerical: 0.566981 analytic: 0.566981, relative error: 4.671357e-08\n",
      "numerical: -4.358320 analytic: -4.358320, relative error: 1.246158e-08\n",
      "numerical: 0.285173 analytic: 0.285173, relative error: 7.512806e-08\n",
      "numerical: -1.796103 analytic: -1.796103, relative error: 2.470459e-09\n",
      "numerical: -0.696280 analytic: -0.696280, relative error: 9.764754e-10\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.363319e+00 computed in 0.132700s\n",
      "vectorized loss: 2.363319e+00 computed in 0.004222s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 6.355961\n",
      "iteration 100 / 1500: loss 6.525238\n",
      "iteration 200 / 1500: loss 6.631721\n",
      "iteration 300 / 1500: loss 6.841696\n",
      "iteration 400 / 1500: loss 6.470692\n",
      "iteration 500 / 1500: loss 6.431544\n",
      "iteration 600 / 1500: loss 6.377031\n",
      "iteration 700 / 1500: loss 6.183520\n",
      "iteration 800 / 1500: loss 7.023232\n",
      "iteration 900 / 1500: loss 6.098649\n",
      "iteration 1000 / 1500: loss 6.895351\n",
      "iteration 1100 / 1500: loss 5.757566\n",
      "iteration 1200 / 1500: loss 6.104108\n",
      "iteration 1300 / 1500: loss 6.113479\n",
      "iteration 1400 / 1500: loss 6.401854\n",
      "iteration 0 / 1500: loss 6.181578\n",
      "iteration 100 / 1500: loss 5.753993\n",
      "iteration 200 / 1500: loss 6.719546\n",
      "iteration 300 / 1500: loss 6.194640\n",
      "iteration 400 / 1500: loss 6.434884\n",
      "iteration 500 / 1500: loss 6.064723\n",
      "iteration 600 / 1500: loss 6.282284\n",
      "iteration 700 / 1500: loss 5.604660\n",
      "iteration 800 / 1500: loss 5.824710\n",
      "iteration 900 / 1500: loss 5.755424\n",
      "iteration 1000 / 1500: loss 5.824656\n",
      "iteration 1100 / 1500: loss 5.795928\n",
      "iteration 1200 / 1500: loss 5.957001\n",
      "iteration 1300 / 1500: loss 5.780529\n",
      "iteration 1400 / 1500: loss 5.848110\n",
      "iteration 0 / 1500: loss 5.580967\n",
      "iteration 100 / 1500: loss 4.906236\n",
      "iteration 200 / 1500: loss 5.377221\n",
      "iteration 300 / 1500: loss 5.459499\n",
      "iteration 400 / 1500: loss 4.967480\n",
      "iteration 500 / 1500: loss 5.425975\n",
      "iteration 600 / 1500: loss 5.447678\n",
      "iteration 700 / 1500: loss 4.887761\n",
      "iteration 800 / 1500: loss 5.188020\n",
      "iteration 900 / 1500: loss 5.087335\n",
      "iteration 1000 / 1500: loss 5.314360\n",
      "iteration 1100 / 1500: loss 5.275506\n",
      "iteration 1200 / 1500: loss 4.983230\n",
      "iteration 1300 / 1500: loss 5.484680\n",
      "iteration 1400 / 1500: loss 5.313075\n",
      "iteration 0 / 1500: loss 5.036094\n",
      "iteration 100 / 1500: loss 5.376890\n",
      "iteration 200 / 1500: loss 5.229051\n",
      "iteration 300 / 1500: loss 4.788689\n",
      "iteration 400 / 1500: loss 5.265309\n",
      "iteration 500 / 1500: loss 5.099388\n",
      "iteration 600 / 1500: loss 4.781165\n",
      "iteration 700 / 1500: loss 5.023285\n",
      "iteration 800 / 1500: loss 5.306539\n",
      "iteration 900 / 1500: loss 4.837207\n",
      "iteration 1000 / 1500: loss 5.092666\n",
      "iteration 1100 / 1500: loss 5.231491\n",
      "iteration 1200 / 1500: loss 5.491158\n",
      "iteration 1300 / 1500: loss 5.349019\n",
      "iteration 1400 / 1500: loss 4.753879\n",
      "iteration 0 / 1500: loss 6.923503\n",
      "iteration 100 / 1500: loss 6.249335\n",
      "iteration 200 / 1500: loss 6.573054\n",
      "iteration 300 / 1500: loss 6.865664\n",
      "iteration 400 / 1500: loss 6.757727\n",
      "iteration 500 / 1500: loss 6.446782\n",
      "iteration 600 / 1500: loss 6.433254\n",
      "iteration 700 / 1500: loss 7.038230\n",
      "iteration 800 / 1500: loss 6.891807\n",
      "iteration 900 / 1500: loss 6.671406\n",
      "iteration 1000 / 1500: loss 6.771528\n",
      "iteration 1100 / 1500: loss 6.506020\n",
      "iteration 1200 / 1500: loss 6.469833\n",
      "iteration 1300 / 1500: loss 6.176409\n",
      "iteration 1400 / 1500: loss 7.054280\n",
      "iteration 0 / 1500: loss 6.660855\n",
      "iteration 100 / 1500: loss 7.049447\n",
      "iteration 200 / 1500: loss 6.526832\n",
      "iteration 300 / 1500: loss 6.698349\n",
      "iteration 400 / 1500: loss 6.559944\n",
      "iteration 500 / 1500: loss 7.268499\n",
      "iteration 600 / 1500: loss 7.098331\n",
      "iteration 700 / 1500: loss 7.646312\n",
      "iteration 800 / 1500: loss 6.894120\n",
      "iteration 900 / 1500: loss 7.400167\n",
      "iteration 1000 / 1500: loss 6.836103\n",
      "iteration 1100 / 1500: loss 6.910232\n",
      "iteration 1200 / 1500: loss 6.864009\n",
      "iteration 1300 / 1500: loss 7.064393\n",
      "iteration 1400 / 1500: loss 7.613655\n",
      "iteration 0 / 1500: loss 21.345240\n",
      "iteration 100 / 1500: loss 21.296201\n",
      "iteration 200 / 1500: loss 21.730683\n",
      "iteration 300 / 1500: loss 21.237765\n",
      "iteration 400 / 1500: loss 21.175377\n",
      "iteration 500 / 1500: loss 21.219709\n",
      "iteration 600 / 1500: loss 21.163760\n",
      "iteration 700 / 1500: loss 21.668889\n",
      "iteration 800 / 1500: loss 21.565938\n",
      "iteration 900 / 1500: loss 20.841075\n",
      "iteration 1000 / 1500: loss 21.390060\n",
      "iteration 1100 / 1500: loss 21.523174\n",
      "iteration 1200 / 1500: loss 21.754409\n",
      "iteration 1300 / 1500: loss 21.359413\n",
      "iteration 1400 / 1500: loss 21.430285\n",
      "iteration 0 / 1500: loss 160.269076\n",
      "iteration 100 / 1500: loss 159.876023\n",
      "iteration 200 / 1500: loss 159.910492\n",
      "iteration 300 / 1500: loss 159.986932\n",
      "iteration 400 / 1500: loss 159.784118\n",
      "iteration 500 / 1500: loss 159.634464\n",
      "iteration 600 / 1500: loss 159.714144\n",
      "iteration 700 / 1500: loss 159.258155\n",
      "iteration 800 / 1500: loss 159.549634\n",
      "iteration 900 / 1500: loss 160.168886\n",
      "iteration 1000 / 1500: loss 160.112844\n",
      "iteration 1100 / 1500: loss 159.888131\n",
      "iteration 1200 / 1500: loss 159.675569\n",
      "iteration 1300 / 1500: loss 159.843617\n",
      "iteration 1400 / 1500: loss 159.870022\n",
      "iteration 0 / 1500: loss 1543.753827\n",
      "iteration 100 / 1500: loss 1541.336319\n",
      "iteration 200 / 1500: loss 1537.990003\n",
      "iteration 300 / 1500: loss 1534.806380\n",
      "iteration 400 / 1500: loss 1531.638168\n",
      "iteration 500 / 1500: loss 1528.936795\n",
      "iteration 600 / 1500: loss 1525.612130\n",
      "iteration 700 / 1500: loss 1522.427776\n",
      "iteration 800 / 1500: loss 1519.835336\n",
      "iteration 900 / 1500: loss 1516.342626\n",
      "iteration 1000 / 1500: loss 1513.531175\n",
      "iteration 1100 / 1500: loss 1509.992512\n",
      "iteration 1200 / 1500: loss 1507.516993\n",
      "iteration 1300 / 1500: loss 1504.699061\n",
      "iteration 1400 / 1500: loss 1501.388372\n",
      "iteration 0 / 1500: loss 15196.642339\n",
      "iteration 100 / 1500: loss 14896.741558\n",
      "iteration 200 / 1500: loss 14601.548834\n",
      "iteration 300 / 1500: loss 14311.977867\n",
      "iteration 400 / 1500: loss 14029.578845\n",
      "iteration 500 / 1500: loss 13751.143078\n",
      "iteration 600 / 1500: loss 13479.146723\n",
      "iteration 700 / 1500: loss 13212.186775\n",
      "iteration 800 / 1500: loss 12950.427363\n",
      "iteration 900 / 1500: loss 12694.397588\n",
      "iteration 1000 / 1500: loss 12443.068657\n",
      "iteration 1100 / 1500: loss 12196.164122\n",
      "iteration 1200 / 1500: loss 11954.843883\n",
      "iteration 1300 / 1500: loss 11717.940379\n",
      "iteration 1400 / 1500: loss 11486.380163\n",
      "iteration 0 / 1500: loss 5.291676\n",
      "iteration 100 / 1500: loss 4.893328\n",
      "iteration 200 / 1500: loss 4.524865\n",
      "iteration 300 / 1500: loss 4.441609\n",
      "iteration 400 / 1500: loss 3.823953\n",
      "iteration 500 / 1500: loss 4.059812\n",
      "iteration 600 / 1500: loss 3.703335\n",
      "iteration 700 / 1500: loss 3.634353\n",
      "iteration 800 / 1500: loss 3.486213\n",
      "iteration 900 / 1500: loss 3.487358\n",
      "iteration 1000 / 1500: loss 3.773422\n",
      "iteration 1100 / 1500: loss 3.219558\n",
      "iteration 1200 / 1500: loss 3.628064\n",
      "iteration 1300 / 1500: loss 3.357363\n",
      "iteration 1400 / 1500: loss 3.391440\n",
      "iteration 0 / 1500: loss 6.858086\n",
      "iteration 100 / 1500: loss 6.022228\n",
      "iteration 200 / 1500: loss 5.371679\n",
      "iteration 300 / 1500: loss 5.164897\n",
      "iteration 400 / 1500: loss 4.381336\n",
      "iteration 500 / 1500: loss 4.320190\n",
      "iteration 600 / 1500: loss 3.828881\n",
      "iteration 700 / 1500: loss 3.845215\n",
      "iteration 800 / 1500: loss 4.008554\n",
      "iteration 900 / 1500: loss 3.719225\n",
      "iteration 1000 / 1500: loss 3.975096\n",
      "iteration 1100 / 1500: loss 3.718877\n",
      "iteration 1200 / 1500: loss 3.732706\n",
      "iteration 1300 / 1500: loss 3.333574\n",
      "iteration 1400 / 1500: loss 3.275713\n",
      "iteration 0 / 1500: loss 5.490212\n",
      "iteration 100 / 1500: loss 4.958758\n",
      "iteration 200 / 1500: loss 4.930103\n",
      "iteration 300 / 1500: loss 4.276817\n",
      "iteration 400 / 1500: loss 3.993043\n",
      "iteration 500 / 1500: loss 4.197166\n",
      "iteration 600 / 1500: loss 4.121354\n",
      "iteration 700 / 1500: loss 4.120173\n",
      "iteration 800 / 1500: loss 3.605000\n",
      "iteration 900 / 1500: loss 3.841773\n",
      "iteration 1000 / 1500: loss 3.495582\n",
      "iteration 1100 / 1500: loss 3.768362\n",
      "iteration 1200 / 1500: loss 3.543002\n",
      "iteration 1300 / 1500: loss 3.480222\n",
      "iteration 1400 / 1500: loss 3.608862\n",
      "iteration 0 / 1500: loss 5.288224\n",
      "iteration 100 / 1500: loss 5.616556\n",
      "iteration 200 / 1500: loss 5.078100\n",
      "iteration 300 / 1500: loss 4.764710\n",
      "iteration 400 / 1500: loss 4.339147\n",
      "iteration 500 / 1500: loss 3.945569\n",
      "iteration 600 / 1500: loss 4.511376\n",
      "iteration 700 / 1500: loss 4.285976\n",
      "iteration 800 / 1500: loss 3.699026\n",
      "iteration 900 / 1500: loss 3.769951\n",
      "iteration 1000 / 1500: loss 3.869263\n",
      "iteration 1100 / 1500: loss 3.750093\n",
      "iteration 1200 / 1500: loss 3.789498\n",
      "iteration 1300 / 1500: loss 3.450679\n",
      "iteration 1400 / 1500: loss 3.590202\n",
      "iteration 0 / 1500: loss 5.619176\n",
      "iteration 100 / 1500: loss 5.081277\n",
      "iteration 200 / 1500: loss 4.784469\n",
      "iteration 300 / 1500: loss 4.452845\n",
      "iteration 400 / 1500: loss 4.205418\n",
      "iteration 500 / 1500: loss 4.091998\n",
      "iteration 600 / 1500: loss 4.389247\n",
      "iteration 700 / 1500: loss 4.150889\n",
      "iteration 800 / 1500: loss 3.921313\n",
      "iteration 900 / 1500: loss 4.075854\n",
      "iteration 1000 / 1500: loss 3.788193\n",
      "iteration 1100 / 1500: loss 3.814980\n",
      "iteration 1200 / 1500: loss 3.758112\n",
      "iteration 1300 / 1500: loss 3.608559\n",
      "iteration 1400 / 1500: loss 3.721986\n",
      "iteration 0 / 1500: loss 7.094593\n",
      "iteration 100 / 1500: loss 6.416061\n",
      "iteration 200 / 1500: loss 6.239480\n",
      "iteration 300 / 1500: loss 5.890314\n",
      "iteration 400 / 1500: loss 5.868782\n",
      "iteration 500 / 1500: loss 5.634739\n",
      "iteration 600 / 1500: loss 5.615732\n",
      "iteration 700 / 1500: loss 5.422017\n",
      "iteration 800 / 1500: loss 5.485782\n",
      "iteration 900 / 1500: loss 5.314161\n",
      "iteration 1000 / 1500: loss 5.169310\n",
      "iteration 1100 / 1500: loss 5.091220\n",
      "iteration 1200 / 1500: loss 5.182806\n",
      "iteration 1300 / 1500: loss 5.183702\n",
      "iteration 1400 / 1500: loss 4.685507\n",
      "iteration 0 / 1500: loss 20.781613\n",
      "iteration 100 / 1500: loss 20.589338\n",
      "iteration 200 / 1500: loss 20.313178\n",
      "iteration 300 / 1500: loss 19.309050\n",
      "iteration 400 / 1500: loss 19.453064\n",
      "iteration 500 / 1500: loss 19.233887\n",
      "iteration 600 / 1500: loss 18.912122\n",
      "iteration 700 / 1500: loss 18.387316\n",
      "iteration 800 / 1500: loss 18.753473\n",
      "iteration 900 / 1500: loss 18.594881\n",
      "iteration 1000 / 1500: loss 18.458051\n",
      "iteration 1100 / 1500: loss 18.262618\n",
      "iteration 1200 / 1500: loss 18.165699\n",
      "iteration 1300 / 1500: loss 18.092284\n",
      "iteration 1400 / 1500: loss 17.870702\n",
      "iteration 0 / 1500: loss 159.868408\n",
      "iteration 100 / 1500: loss 153.688066\n",
      "iteration 200 / 1500: loss 148.341807\n",
      "iteration 300 / 1500: loss 143.332560\n",
      "iteration 400 / 1500: loss 138.391553\n",
      "iteration 500 / 1500: loss 134.128089\n",
      "iteration 600 / 1500: loss 129.455366\n",
      "iteration 700 / 1500: loss 125.221879\n",
      "iteration 800 / 1500: loss 121.010709\n",
      "iteration 900 / 1500: loss 117.115902\n",
      "iteration 1000 / 1500: loss 113.147988\n",
      "iteration 1100 / 1500: loss 109.611468\n",
      "iteration 1200 / 1500: loss 105.886376\n",
      "iteration 1300 / 1500: loss 102.481620\n",
      "iteration 1400 / 1500: loss 98.964813\n",
      "iteration 0 / 1500: loss 1535.530958\n",
      "iteration 100 / 1500: loss 1099.517409\n",
      "iteration 200 / 1500: loss 787.384331\n",
      "iteration 300 / 1500: loss 564.049777\n",
      "iteration 400 / 1500: loss 404.372604\n",
      "iteration 500 / 1500: loss 290.070485\n",
      "iteration 600 / 1500: loss 208.083162\n",
      "iteration 700 / 1500: loss 149.538614\n",
      "iteration 800 / 1500: loss 107.661059\n",
      "iteration 900 / 1500: loss 77.664617\n",
      "iteration 1000 / 1500: loss 56.184122\n",
      "iteration 1100 / 1500: loss 40.845422\n",
      "iteration 1200 / 1500: loss 29.845435\n",
      "iteration 1300 / 1500: loss 22.011322\n",
      "iteration 1400 / 1500: loss 16.320900\n",
      "iteration 0 / 1500: loss 15471.260422\n",
      "iteration 100 / 1500: loss 536.905482\n",
      "iteration 200 / 1500: loss 20.736704\n",
      "iteration 300 / 1500: loss 2.902931\n",
      "iteration 400 / 1500: loss 2.281756\n",
      "iteration 500 / 1500: loss 2.263287\n",
      "iteration 600 / 1500: loss 2.275349\n",
      "iteration 700 / 1500: loss 2.263636\n",
      "iteration 800 / 1500: loss 2.263879\n",
      "iteration 900 / 1500: loss 2.271519\n",
      "iteration 1000 / 1500: loss 2.260694\n",
      "iteration 1100 / 1500: loss 2.268341\n",
      "iteration 1200 / 1500: loss 2.263970\n",
      "iteration 1300 / 1500: loss 2.264995\n",
      "iteration 1400 / 1500: loss 2.282863\n",
      "iteration 0 / 1500: loss 5.187110\n",
      "iteration 100 / 1500: loss 2.422085\n",
      "iteration 200 / 1500: loss 2.163646\n",
      "iteration 300 / 1500: loss 1.882446\n",
      "iteration 400 / 1500: loss 2.017853\n",
      "iteration 500 / 1500: loss 2.145709\n",
      "iteration 600 / 1500: loss 1.808856\n",
      "iteration 700 / 1500: loss 1.999407\n",
      "iteration 800 / 1500: loss 1.751248\n",
      "iteration 900 / 1500: loss 1.970219\n",
      "iteration 1000 / 1500: loss 1.823818\n",
      "iteration 1100 / 1500: loss 1.877924\n",
      "iteration 1200 / 1500: loss 1.922518\n",
      "iteration 1300 / 1500: loss 1.971458\n",
      "iteration 1400 / 1500: loss 1.719362\n",
      "iteration 0 / 1500: loss 5.099777\n",
      "iteration 100 / 1500: loss 2.429827\n",
      "iteration 200 / 1500: loss 2.456627\n",
      "iteration 300 / 1500: loss 2.120752\n",
      "iteration 400 / 1500: loss 2.051141\n",
      "iteration 500 / 1500: loss 2.019973\n",
      "iteration 600 / 1500: loss 2.047032\n",
      "iteration 700 / 1500: loss 1.903406\n",
      "iteration 800 / 1500: loss 1.911236\n",
      "iteration 900 / 1500: loss 1.938590\n",
      "iteration 1000 / 1500: loss 1.811941\n",
      "iteration 1100 / 1500: loss 1.895495\n",
      "iteration 1200 / 1500: loss 1.824059\n",
      "iteration 1300 / 1500: loss 1.985707\n",
      "iteration 1400 / 1500: loss 1.995300\n",
      "iteration 0 / 1500: loss 6.534876\n",
      "iteration 100 / 1500: loss 2.418010\n",
      "iteration 200 / 1500: loss 2.203853\n",
      "iteration 300 / 1500: loss 2.252771\n",
      "iteration 400 / 1500: loss 2.155645\n",
      "iteration 500 / 1500: loss 2.009021\n",
      "iteration 600 / 1500: loss 2.115710\n",
      "iteration 700 / 1500: loss 2.105632\n",
      "iteration 800 / 1500: loss 1.911702\n",
      "iteration 900 / 1500: loss 1.829814\n",
      "iteration 1000 / 1500: loss 1.772860\n",
      "iteration 1100 / 1500: loss 1.891239\n",
      "iteration 1200 / 1500: loss 1.761220\n",
      "iteration 1300 / 1500: loss 1.729159\n",
      "iteration 1400 / 1500: loss 1.837961\n",
      "iteration 0 / 1500: loss 5.545460\n",
      "iteration 100 / 1500: loss 2.383482\n",
      "iteration 200 / 1500: loss 2.240360\n",
      "iteration 300 / 1500: loss 1.957858\n",
      "iteration 400 / 1500: loss 2.080196\n",
      "iteration 500 / 1500: loss 2.095199\n",
      "iteration 600 / 1500: loss 1.989547\n",
      "iteration 700 / 1500: loss 1.896132\n",
      "iteration 800 / 1500: loss 1.914876\n",
      "iteration 900 / 1500: loss 1.891785\n",
      "iteration 1000 / 1500: loss 1.782319\n",
      "iteration 1100 / 1500: loss 1.914449\n",
      "iteration 1200 / 1500: loss 1.797566\n",
      "iteration 1300 / 1500: loss 1.887992\n",
      "iteration 1400 / 1500: loss 1.857911\n",
      "iteration 0 / 1500: loss 6.600392\n",
      "iteration 100 / 1500: loss 2.552353\n",
      "iteration 200 / 1500: loss 2.445621\n",
      "iteration 300 / 1500: loss 2.204048\n",
      "iteration 400 / 1500: loss 2.304780\n",
      "iteration 500 / 1500: loss 2.043579\n",
      "iteration 600 / 1500: loss 2.174131\n",
      "iteration 700 / 1500: loss 1.967179\n",
      "iteration 800 / 1500: loss 1.972862\n",
      "iteration 900 / 1500: loss 2.132791\n",
      "iteration 1000 / 1500: loss 1.925089\n",
      "iteration 1100 / 1500: loss 2.103443\n",
      "iteration 1200 / 1500: loss 1.902369\n",
      "iteration 1300 / 1500: loss 1.916337\n",
      "iteration 1400 / 1500: loss 2.011758\n",
      "iteration 0 / 1500: loss 7.703856\n",
      "iteration 100 / 1500: loss 3.924874\n",
      "iteration 200 / 1500: loss 3.508095\n",
      "iteration 300 / 1500: loss 3.230365\n",
      "iteration 400 / 1500: loss 3.279616\n",
      "iteration 500 / 1500: loss 3.077976\n",
      "iteration 600 / 1500: loss 2.851579\n",
      "iteration 700 / 1500: loss 2.962742\n",
      "iteration 800 / 1500: loss 2.837409\n",
      "iteration 900 / 1500: loss 2.589713\n",
      "iteration 1000 / 1500: loss 2.575434\n",
      "iteration 1100 / 1500: loss 2.314652\n",
      "iteration 1200 / 1500: loss 2.518098\n",
      "iteration 1300 / 1500: loss 2.595844\n",
      "iteration 1400 / 1500: loss 2.371273\n",
      "iteration 0 / 1500: loss 21.095649\n",
      "iteration 100 / 1500: loss 10.851168\n",
      "iteration 200 / 1500: loss 6.876293\n",
      "iteration 300 / 1500: loss 4.646245\n",
      "iteration 400 / 1500: loss 3.294568\n",
      "iteration 500 / 1500: loss 2.776860\n",
      "iteration 600 / 1500: loss 2.308273\n",
      "iteration 700 / 1500: loss 2.029295\n",
      "iteration 800 / 1500: loss 2.058320\n",
      "iteration 900 / 1500: loss 1.847206\n",
      "iteration 1000 / 1500: loss 1.864268\n",
      "iteration 1100 / 1500: loss 1.974542\n",
      "iteration 1200 / 1500: loss 1.820060\n",
      "iteration 1300 / 1500: loss 1.780205\n",
      "iteration 1400 / 1500: loss 1.830239\n",
      "iteration 0 / 1500: loss 158.653712\n",
      "iteration 100 / 1500: loss 2.530633\n",
      "iteration 200 / 1500: loss 1.970958\n",
      "iteration 300 / 1500: loss 2.003489\n",
      "iteration 400 / 1500: loss 1.929050\n",
      "iteration 500 / 1500: loss 2.003156\n",
      "iteration 600 / 1500: loss 2.002028\n",
      "iteration 700 / 1500: loss 2.043605\n",
      "iteration 800 / 1500: loss 1.948467\n",
      "iteration 900 / 1500: loss 2.021324\n",
      "iteration 1000 / 1500: loss 2.104363\n",
      "iteration 1100 / 1500: loss 2.003762\n",
      "iteration 1200 / 1500: loss 1.942239\n",
      "iteration 1300 / 1500: loss 2.042610\n",
      "iteration 1400 / 1500: loss 2.074338\n",
      "iteration 0 / 1500: loss 1521.297455\n",
      "iteration 100 / 1500: loss 2.175118\n",
      "iteration 200 / 1500: loss 2.228403\n",
      "iteration 300 / 1500: loss 2.206091\n",
      "iteration 400 / 1500: loss 2.213071\n",
      "iteration 500 / 1500: loss 2.167893\n",
      "iteration 600 / 1500: loss 2.201801\n",
      "iteration 700 / 1500: loss 2.190207\n",
      "iteration 800 / 1500: loss 2.143775\n",
      "iteration 900 / 1500: loss 2.267528\n",
      "iteration 1000 / 1500: loss 2.217909\n",
      "iteration 1100 / 1500: loss 2.238546\n",
      "iteration 1200 / 1500: loss 2.190909\n",
      "iteration 1300 / 1500: loss 2.211353\n",
      "iteration 1400 / 1500: loss 2.160283\n",
      "iteration 0 / 1500: loss 15329.586975\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.463603\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.248066\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.475936\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.528668\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.501308\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 7.290980\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 20.519214\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 158.450706\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1535.391919\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15444.737748\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.577188\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.980090\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.708509\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.220107\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.676008\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.775423\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 20.941848\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 159.367847\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1544.824181\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15339.339815\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.361068\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.198023\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.218494\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.707026\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.941624\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 7.399263\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 22.538490\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 158.072716\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1531.226530\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15445.652459\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 4.905659\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.254765\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.783746\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.470626\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 7.020233\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 7.278071\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 20.848717\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 156.907900\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1543.810364\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15560.221587\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.780719\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.771481\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.386991\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.919616\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 4.954407\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.494274\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 20.420565\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 156.479672\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1545.116276\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15407.041804\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 4.750416\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.306110\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.468125\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.088653\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.988787\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.922047\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 20.520364\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 158.595849\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1533.497446\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15463.297578\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.155026\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 7.193065\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.146048\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.156953\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.976384\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 7.214438\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 20.533983\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 157.955730\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1537.788566\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15168.878602\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "lr 1.000000e-10 reg 1.000000e-03 train accuracy: 0.095020 val accuracy: 0.104000\n",
      "lr 1.000000e-10 reg 1.000000e-02 train accuracy: 0.099490 val accuracy: 0.088000\n",
      "lr 1.000000e-10 reg 1.000000e-01 train accuracy: 0.100531 val accuracy: 0.094000\n",
      "lr 1.000000e-10 reg 1.000000e+00 train accuracy: 0.096510 val accuracy: 0.113000\n",
      "lr 1.000000e-10 reg 1.000000e+01 train accuracy: 0.081673 val accuracy: 0.071000\n",
      "lr 1.000000e-10 reg 1.000000e+02 train accuracy: 0.108306 val accuracy: 0.105000\n",
      "lr 1.000000e-10 reg 1.000000e+03 train accuracy: 0.098143 val accuracy: 0.112000\n",
      "lr 1.000000e-10 reg 1.000000e+04 train accuracy: 0.112224 val accuracy: 0.097000\n",
      "lr 1.000000e-10 reg 1.000000e+05 train accuracy: 0.106857 val accuracy: 0.097000\n",
      "lr 1.000000e-10 reg 1.000000e+06 train accuracy: 0.095653 val accuracy: 0.085000\n",
      "lr 1.668101e-08 reg 1.000000e-03 train accuracy: 0.184061 val accuracy: 0.203000\n",
      "lr 1.668101e-08 reg 1.000000e-02 train accuracy: 0.166816 val accuracy: 0.161000\n",
      "lr 1.668101e-08 reg 1.000000e-01 train accuracy: 0.173755 val accuracy: 0.177000\n",
      "lr 1.668101e-08 reg 1.000000e+00 train accuracy: 0.165367 val accuracy: 0.175000\n",
      "lr 1.668101e-08 reg 1.000000e+01 train accuracy: 0.171633 val accuracy: 0.161000\n",
      "lr 1.668101e-08 reg 1.000000e+02 train accuracy: 0.161367 val accuracy: 0.166000\n",
      "lr 1.668101e-08 reg 1.000000e+03 train accuracy: 0.181327 val accuracy: 0.191000\n",
      "lr 1.668101e-08 reg 1.000000e+04 train accuracy: 0.188857 val accuracy: 0.182000\n",
      "lr 1.668101e-08 reg 1.000000e+05 train accuracy: 0.277490 val accuracy: 0.280000\n",
      "lr 1.668101e-08 reg 1.000000e+06 train accuracy: 0.259714 val accuracy: 0.265000\n",
      "lr 2.782559e-06 reg 1.000000e-03 train accuracy: 0.384082 val accuracy: 0.362000\n",
      "lr 2.782559e-06 reg 1.000000e-02 train accuracy: 0.382245 val accuracy: 0.374000\n",
      "lr 2.782559e-06 reg 1.000000e-01 train accuracy: 0.383898 val accuracy: 0.361000\n",
      "lr 2.782559e-06 reg 1.000000e+00 train accuracy: 0.385714 val accuracy: 0.362000\n",
      "lr 2.782559e-06 reg 1.000000e+01 train accuracy: 0.386469 val accuracy: 0.356000\n",
      "lr 2.782559e-06 reg 1.000000e+02 train accuracy: 0.400388 val accuracy: 0.387000\n",
      "lr 2.782559e-06 reg 1.000000e+03 train accuracy: 0.405204 val accuracy: 0.396000\n",
      "lr 2.782559e-06 reg 1.000000e+04 train accuracy: 0.350082 val accuracy: 0.346000\n",
      "lr 2.782559e-06 reg 1.000000e+05 train accuracy: 0.267776 val accuracy: 0.274000\n",
      "lr 2.782559e-06 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.396000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.logspace(-10, 10, 10)\n",
    "regularization_strengths =  np.logspace(-3, 6, 10)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        model = Softmax()\n",
    "        model.train(X_train, y_train, learning_rate=lr, reg=reg, \n",
    "                                      num_iters=1500, verbose=True)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        training_accuracy = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        validation_accuracy = np.mean(y_val == y_val_pred)\n",
    "        results[(lr, reg)] = (training_accuracy, validation_accuracy)\n",
    "        if validation_accuracy > best_val:\n",
    "            best_val = validation_accuracy\n",
    "            best_softmax = model\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.376000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF8CAYAAADrUz6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsvXnMbXua1/X81rzWnod3Pufcc4eqpqqrjY0BYkxLJxKJ\ndBMUEg1RsTUYiUNrjKAdSWhjC0roPxASYxCaBFCJLTGi0RiHRBQMURuwq4rqW7fuPeM77Xla81r+\ncU7t73cfqu89p/c+763q+3ySm6y737XX+s37d57v73keU9e1KIqiKIqiKL82rM+7AIqiKIqiKD/I\n6GZKURRFURRlD3QzpSiKoiiKsge6mVIURVEURdkD3UwpiqIoiqLsgW6mFEVRFEVR9kA3UyJijPlx\nY8zTz7sciqIAY8wnxpjf9j0+/zFjzLfe8Fl/3hjzc4crnaIoIjq3votuphRF+YGiruu/Vtf1D33e\n5VDull9tc60o3w/oZkpRfhWMMc7nXQblzdA+U5QffH4Q5/EXajP18l82P2OM+YYxZmqM+QVjTPA9\n7vt3jDEfGWOWL+/9J+hvP2WM+T+MMX/i5TM+Nsb8Y/T3jjHmzxpjLo0xz4wxP2eMse+qjgowxtw3\nxvwVY8ytMWZsjPnTxpj3jTH/68v/Hxlj/pIxpkvf+cQY828bY/6OiKx/ECf1rzN+06vz9VVZ/nv1\nmTHmR40x/+/LOfyXReTvmefK58ebzk1jzF8QkQci8leNMStjzB/6fGvwxeXT5pYx5ieNMX/LGDMz\nxvx1Y8zfR387N8b81y/7/GNjzE/T337WGPOLxpi/aIxZiMhP3WmlDsAXajP1kn9aRH67iLwvIl8W\nkT/8Pe75SER+TEQ6IvLvichfNMac0d9/i4h8S0SGIvLHReTPGmPMy7/9eREpROQDEflREflHReT3\nH7wWyqfycgP734nIIxF5KCIXIvJfiogRkT8mIuci8hURuS8iP/vK13+viPyEiHTrui7upsTKr8Lr\nzFcR6jN5sa79NyLyF0SkLyL/lYj8nrdeUuW1+LXMzbqu/1kReSwiv7Ou62Zd13/8zguuiDHGk19l\nbhljflRE/pyI/EsiMhCR/1RE/ltjjG+MsUTkr4rI35YX/f2PiMi/YYz57fT43yUivygv5vBfupMK\nHZK6rr8w/4nIJyLyB+j/f4e82Dj9uIg8/ZTv/S0R+V0vr39KRL5Nf4tEpBaRUxE5EZFUREL6++8V\nkf/t8677F+0/EfkHReRWRJzPuO8fF5FfemWM/Aufd/n1v9efr6/2mYj8wyLyXEQMffbXReTnPu86\n6X97z83f9nmX/4v836fNLRH5T0Tk33/l/m+JyG+VFwaIx6/87WdE5BdeXv+siPzvn3f99vnviyhh\nPKHrR/LiX0E7GGN+n4j8m/LiX00iIk15YYX6LlffvajrevPSKNWUFzt1V0QuYagS65V3KnfDfRF5\nVL9iWTLGnIjIn5QXlseWvOif6Svf1f76/uEz5+v3uO9cRJ7VL1dp+q7y/cE+c1P5fPm0ufWOiPxz\nxph/jf7mvfxOKSLnxpgZ/c0Wkb9G//8Dve5+EWW++3T9QF7ssrcYY94RkT8jIv+qiAzquu6KyC/L\nCxP0Z/FEXlimhnVdd1/+167r+ocPU3TlDXgiIg++x5mnPyovLIk/Utd1W0T+Gfl7+7YW5fuFT52v\nBPfZpYhckPT+3e8q3x/8WuemzsvPn0+bW09E5D+g375uXddRXdf/xcu/ffzK31p1Xf8Oes4PdP9+\nETdT/4ox5p4xpi8i/66I/OVX/t6QF516KyJijPnnReRrr/Pguq4vReR/EpGfN8a0jTHWy0OVv/Vw\nxVdek78pLyb+f2iMabw8uPwPyYt/8a5EZG6MuRCRP/h5FlL5TD5rvn4v/oa8OLf408YY1xjzu0Xk\nN7/NQipvxK91bl6LyHt3W1TlFT5tbv0ZEfkDxpjfYl7QMMb8hDGmJS/6fPnSUSQ0xtjGmK8ZY37T\n51SPg/NF3Ez95/Jiw/MdeXH+YifYWF3X3xCRn5cXg+ZaRH5ERP7PN3j+75MXps1vyAsT9S+KyNmn\nfkM5OHVdlyLyO+WFI8BjEXkqIv+UvHAo+I0iMheR/15E/srnVUbltfjU+fq9qOs6E5HfLS/ON07k\nRb9rP3+fsMfc/GMi8odfeor9W3dXYuW7fNrcquv6/xaRf1FE/rS8+O379sv7vtvnPykif7+IfCwi\nIxH5z+SFk9evC8yu9PnrG2PMJyLy++u6/p8/77IoiqIoivLrgy+iZUpRFEVRFOVg6GZKURRFURRl\nD75QMp+iKIqiKMqhUcuUoiiKoijKHtxp0M7/+E/9X1sz2GaSbj+/WSBWl23j8zpsbq/DqIV7yJhW\nFgh3UZeX2+vV0sV36yU+N3AeCOzx9jpfhyiPm2yveyO8LDmudurj1b3tdUrb0qBEGrBZGW+vjyvc\nVAwz3E/hVjYxym1lq+11P8I9qY86+zHaS0yEcrtoOyvyt9f/+h/6ydeJl/WZ/NGf/iPbhglDtJF3\ngfcGt2j32wLlzEqkKuwV6+11baO+j28Rz69zjHotksX22s/62+svfel4ez1PUB5rlm+vly6e07W2\ncVdFRGTt4zvhCP1XNLdp+8RroEw2hVmxcowLJ8I4yld4n23h87qzwXWC8bHJqC1u0WcrB2NlUuKe\nn/+Tf+IgfSki8kd+/ie2DdDptbef9wco99pD32YTzBE3QH/6BfpnQXNtU21T6Ym9QpvmazynWpXb\n6xVeJa2osb22coyp08G97XVZYN6IiIxKtPEqR9zHlNo7KlC+ysM4aTj0rAqfb2q0S2ChT5YZ6nBU\nYI7nfRrDtN5taP2qCoz5n/mD/8tB+vM/+pf/ye0bmhZiDS9dtEmwRlvHFsrsZChC9wzjuieYa5Ml\n2rOovO21qfDMKPzK9to9v9lelyPck8ypX1JKX9raTaPYyNFnTgu/Aw0bz8pLrI+Zh/r4MfrJylDW\nsxb643aK/otClOObFd5bTjEmyhpjPMsxNq/G39pe/6n/8W8cbG7+nh/78W1/vvOjiAzRiTCP+jHq\nlnfwuTGYL/MNtXEH7dUfoL2iEr9pRYl6Wh1aB55NcG1hTHk+1jsxaK/mffSZiEgwxbxYLG6317Yz\n2F5na7zbxlIoto2yzsYYn5mgn7s5vjDzsF42aiwqfgPlnl+jPE9Wz7bXIc3NP/0L/8Nn9qdaphRF\nURRFUfZAN1OKoiiKoih7cKcy3zVJFG4Bia1NpsJCYLobljCzJeM5HuSTSZPM1amBeThvwExYLWGK\nbfVg6lzGMAeeHsFEeU9gorxuk2yV76aJMj5Mn60WmZlpi+rdwvwY2zAnn1WQpVwf7bJuwuTYTiEr\niIey5jnqtnJQ7jDHcwZkAm70YT49FIM+6uudn26vsw3axO5DSuvn6KdNib6MBH0zNqjjBy2MA48k\nnFYfZmif5AkvgnaSTTBuvBDfbTdg2k9fMdomV/iOuJB2rAJmcrfEs1IX/efVNO5uqW9CtFHrDC+M\nDOqwzFEmTlQWeWgX6WIM5U/lrVCuUbfqGH01c1FW6xJzpzCYv2tyYrFHKLdN89dqQFJr1HhXnqHt\nvCGuqxGkoYD+zWcbyE23U5ItSpRZRMQuMJbqHGPSy2jJi/D9yuAd13OUu+WSBGYwv1YWesvKMQfr\nI7RFsETbZR6kkUYT68jjK9IzD0Sz+3B7bdZYK32SZsuQ5pGL+gY1xn47wRpS3afxOEPfzCu0Z41h\nKqvW4+316RptGHRJaqE1avoIqRMLqGsiImLTPPLbNEY2KNOmgSxD1YiOY9BlGOL+lNaaeoA+u36M\nL5wOcM9tiDYyFergUrab7vFX5W1wfoGMSraP8p0ISVUGvxUtkiE/XKFTUpItW+nR9jooMT+WG6TO\nazRQt9EM9R/n1BaC8RLQ757bwpwoxlhDRURiG/PO9lAfd4xjOs8xVCXoYN6FHWSFspr4vEVHXBLK\nSrO+oaMDEd4VrVFuEbRL26c1Je3Jm6CWKUVRFEVRlD3QzZSiKIqiKMoe3KnMJwtIA2EFU/dzkq16\nLmSPNXn9FGROnpE04hyT6fYKz2xFMNGtyCyZGpJwSphrL5cw2zeG+K6zgCS1aZPsJiLdBLbI0RzS\nRVDgc38Ks/SQ1LasRdLFEvc3yAsip+eLj8+7KXnWxP8f6lOgLZ5V8JB8n5wsDsVsBJPskIaRaZDX\nS4T2rRzoU/6KpBBYmOW+hxSGG/LU6fhod5u1sAb6e75CPyVk/s0SlK0gr52aZFMREYfeNyc58AMX\n735O3maNE8g2ZQPeKkmMdxQ1xruM0fkz63p7bZFJ3rXRGNGQPCQNvvuJAzP8IeldnG+vC5JPwylk\ngg2ZwJMZBpVJcN2MYDJPLci/TXI6rQL0id3Du6wSfXL+JXihlc/Q7pWP8mTk2bWYoM9FRIYr/L/X\nJ6/gEPMx9PH5qsbAGqSYOwsXZZp66JMj8pZt9SAfbFxII45BuQcp+tBpnGyv6y76/1BY5LXau0C9\n1gHKkNLRCjtBm7gh7qnQBULNLrGQt+sS/degZ140SAb3MG6mM0g+IR2NGISQskYtnuQiNcnrbo3v\nTFvU1s+xZgc+yrEpyVMtwNiM6XiBQ+5ig2P8Psws9J8dYx3wHTrKEI22193qzWSh1yXskmdcjLZZ\nhahbGWG8z2pI6kc2yh27qE/tQP79zhyfBySRlYIB4NQf4bqHPkho7GQ22tdf4Do4pUVeRArB2ikL\nlKM6wbgdpOTxnJEnfwvzrqY1yAqw/lcu/b54aLsixHrsTdDnCR0Fud/HuE1LWr9fA7VMKYqiKIqi\n7IFuphRFURRFUfbgTmW+NIEJNSZpoB3BzNgmV7iaPPJiD6a+DpmZwxXMnnMKHpgIpL04gPTSr/H8\nKUkPzQ5M7yaDiXFNAS8dCiopIjKGNVXqGCbndY7yNVsISlZ5ZH6PYZZck+dWUUICmlMAwHNyPig6\nMI22c9zzfE6egBRAbX14JUG+OYUJ9EdO8a4WeYbMFvCwWW/QPqEDM2xtcP8FyZFRE1Km3Saz8hz1\njTM8Z0HfvSnRWL0I77V7aLeFu+v9dUYB4zoWTMljoSCy9B23wnisKUhg/wKD4voJJLmqhTGVk+dZ\ng64rem9G3mJxjufYN7sepYeiR55Lmwztt74l2aMkuZX+HWZR0NYN9achb6PERbmtHHONgy3WJG02\nErTjok9Bap+hPIb6ZlDuyraeje+c+rReUPDfLMB1c4L5eHsG6cbJ0Q/nBepfUeDceI11obRQvtqC\nbG3OICXUPKZakBUOxeoGZT59gHlk7lGfLVB+N2BJBRJMQVKLK2ir988vttezLtpqdkMeXOyB2ySJ\nrEB/3y7RDsMmzYNi18Pxaky/A3O8L3dQpmoDqTUn6WgYoBzGxRwvMQQlm0Eiyy3UIbDo+W2U1XYx\nnjY0J2T5diR4KSgQ8Bx6a/IOeTbS8ZA+BQieNdEuoUUBVsk5c5Xj99Emz+Q5yXFNQ0Ggx+grE+D3\nbUhHa8oheY5Wu+6Z9RLtWp6g/RyaFw0fc8d9l45wfIJxm/WwXlQOeQuTtJtHdMxmg2NDlxWOqRy7\naJc+/RaPXznW81moZUpRFEVRFGUPdDOlKIqiKIqyB3cq8xmHAmyWFKxrjetpQAEAx7guKDdQ2CQZ\nYoHvxm2Y60oKytVdkIccyRkl5eBbUa41lvMsMp8m5IQgIlJlJC30ITH22zDFlgnM+LUNs/mgiftD\n8ixJpgimFg5gQq2XlKvIxv3XLuWqalBurCaZOinI6aE4ztDuPnlaegEkhtaKApmmFD3PwLNvab2z\nvf6IPEAiyq9kU2A8z4Y59yrB5/MxBW2j4K3zirxTbJShiHeH/srHc8s+57zCc72H6INwgncXJxSY\nlQIUNkkKjMnUPfbQT86KvF42JE2nuL4tIJcucwRDPCQ25a3yJpg70xDyXDKGl1uzgveVf4E6eBme\nMyVvxvkcukqTgrYOyZ3TmkOGqUku9g3kk8sM3mBlRjLaCnNFRKTKML+OEsgHLFVGo0+21yzgJ2v0\nf+mT122EILQuPecqI28o8jwLA0hS375Ge10cQVbynV158hB0eu9vr1cV2tqfkOdzQeOoQd7LJcas\n/wTzJfkaPncuKXBkgCCKefXL2+v5JWQUe0P50VKMidYQHqRxhnUyTHel7KsV2j2g9S5u0e+AoTFF\n8y4xFJxyg3uKjykAb4MCHEcYg0v6fQhnGONLko7kKX4U8vLwfSki0mzS+t3CO7IOxuCXItTzKXm/\nhSnqPLfoXEtOHoINeO2x5JdRnlxj8Hk+xNrpJHjXho/x0PP7nV3vzIR+I8qacmjGGANBmwKD3uAd\ngYt3hxXGVZGhf6oc46rpY7248uk3Jcf6sKYg3Y5HUuj7u16In4VaphRFURRFUfZAN1OKoiiKoih7\ncKcyH3tAlTnyO4VN8u5KYH6rKf+bXUFiccirajOAKY6dBppHFKyRg3w6MOnZEQUMc8hdjiSfvCYJ\n4xSeMSIixsAUWZAXxDKBjHXkwoRa0971SQPlsyjvkaTw5mt18L5eAnNlSnnuuiSXGpfyCuWQW/IV\n5yE6DHkXJuYj8ni6WbGsBvP8aYDyLErICpeC/htQsLVJgXYw5FGWhxgH9RJm3uoB55BD2WYL3J+m\n6ItmB2NRRCRbY4wkwuZwCua6wjuujp5tr72YTMMezNYxeS1mJP8dx6hPEaH+qzkkEIdkqoqCBBbd\n3XIfiqtbmPHblMsyH1Kb9cnTliST0Qxjv0NzuaJ51CaJ1Z2iDtMS37ViyDtuH23Uien5NIeW34TE\n4l3smuQjkt2DlGQZmqez55jbeYM8uigwoEdrh2VBSrli+SxF/x+FqOd6Rf9WtXB/SvK3Hx1eGvJP\n0AdCcvmtQwGISRavUwoQa1OA04fkUTvGeF+t0CbLksZ4i+SSCdq2uca7cgoKObnCXGn2Mf6qYne9\ncmzynLVJ5qHAk+cuxmxWQ4I/Osf7XJKghY51hCnWU+7XaoLP19QWg0sKHNrEnC3jtxAdWUQ2QkcN\nOmgbj343Jh55/C3x2xqT52FJxwsywbho5TimMB+hPmkH9YxJXi1JXh02IIuaGvMxSSBxX092bTZN\njzx16ThOM4Q0nMRo72xNwUBJ8bTIEzhYYt61+hif1zl+C4oExwWsEmWwaG2y6AVB/mbHY9QypSiK\noiiKsge6mVIURVEURdmDO5X5PB+SSdrD9fNbmFy7ZPVuUFA3OcY9+RA3dVYwgU5uYWZ0A8oL1sGe\n0VDeIo886iY2ZLQOBe1c2DA3etWuGdcSmBBNRUHn8ndxjwUTahFQ9MwZypqRR0QxwDuGM5gf0wRl\nsigoaLeH66sx5Z5K4QEWRofPGXVKHi3LFO3bCtBnHgVnDEiyvM3QJgF5yciAg65yQDbc4pK0Gjfg\nCWiluIlURFk2MQ76LmSa6+Vu3iX3ATxJyjnlufIhPTkFzOfVDcp9blHQSso7F6YYszPB+LJDmI/P\nSO69JWmquKZgeA1IGIsR6nNIFmvyvBxQUNEI/bYYYcyuScKkWKgyLTGva6i84rUxxpcuyZnU1tUM\nY8cl8/8Hja/icwftctXAPWfub9ypz/ACkk5ZoD7TW/ThnCTGZoxyGI/mJvVndYP5W1Cw1RYFHlxZ\neKZD3nxHJfUbeSSxV/OhsNe0LpG04ZSoY5s8sqoRyUULknzIk3lAeeDymHLCUf6yIXuLscebA4lw\nxh67KQVRXOGeLgVlFhFp2RQAco7vFxmkreU9rAURj00KXlz5+HxKOWAvn6I+wxDtso7IK3uB+6ch\nyeAp1o2o/XZsE+TsLtcTTLZQqG8nFGC1gXWuzXnqbHx3tYGktqK+8i4g24WU33JMHpkWjdmC8unV\nJLWWV+izW2/XOzMiSbd/jHdPYwocTL8pQYq2jymAqaFgq90urZ2UpzUQ/BgsM7TXLCSvxQB7Ajun\nHxtad18HtUwpiqIoiqLsgW6mFEVRFEVR9uBOZb7CgwTgUv6kfgfm8OmUTJctmPQaKQX6GsM0mEQw\n6UXv4ZnpHKbFkvLxdSjAnk2m65ACjq0oh5FLef3WrwQGbGYk+7Vw3WiTRxsFKBuRd0gwx3XSIm8F\napdNG+92Dcp6QvnPnl6i/lcbtJ0rMD/70a7Z/BBU1F5Sk7n1HgKmlZSzTjpok84a8oGQaTcvSOYj\nj8VyiOt6DZPslLxNbtbsUQd5bZxC8hjHMIWXxa4J14th3i42aF/XRvkGFcZUSuNobpPsMSOvMpck\nyW9AxlifkSdn673t9SkFtfX6kNcSC58fP3g7U9Yp0TZtQVusyIs2J0+1+Qoymk0eq+sE0ojbwDMN\nefB4Hr77/PnXUQjq/yEF7RwVj1C2CG39D2SQM9yQNEURkTHqUDl47o2gfBENw1WLZE4P8lM7pICD\nlPvzbENrDUkGfkpzuY3gmXEFqaKq8a6us5vv8xBUBu8qKQhjlVJ+QAdleDrHGmJKyNHhLer4kGXT\nJtbBgiQ5QzkamxXem8RozxV5vmY93HPfxXi/JtlcRKRRUM43H+vORvC7kS3h/bt0SdprfbS9jmjd\nKTLUv9PH2LykvICbCkFqj8kr0p/Rb8sMZeVl7ZBwINFWE/1zu8QLA4e89gK0UUa5CcOIjlRYWP8a\nLfRPl47EpJR/cj6lNcvgObe0Dg6pDMEPQ9Z/b4wyi4gYyr/rBFifuz1I8HJNHq/HkMin1+hnobLG\nFLy7CLF2JPOPt9c5reXtiLzLN3jOqqbf+wrj63VQy5SiKIqiKMoe6GZKURRFURRlD+5U5uusYFpc\nk4dGvC7pLgpQaGCSXyfY93kZzNhd8taIc9wTkRl77cJUX3Vx/5o8hiobpk6PzMrTFUy67EkgIrIo\nUb4eyQdlivvsGnXm/EbSocCABibKysHn9SU8ZSwfcsuYgv6ZHFJKL6JgozHa9Mgc3v6cUqC/JCKT\nbIF2qMgzZlTANOwu0R85BWa9Je+RSQpJxVqjDS0K6Dci8+yzDFLFbAYTs0cePzOSoJavqCvnGdq3\nlZJU1eRAs/hSmkGWmDoYj6GFct8LYFZudihHY4Xn5zOYkrtjtItDuSgDUmlvniEg7CGJemjjok1j\nMMX4ahVoo4rahQNy9sgd97ZAwX2PxvgcdTu6gCycPoFJvkHBTDl0XkZTyCYz/3pNAWtFpEwg+5Xk\nkdumIIYVBfbNSbE/vbiHMt3gOQ7N0+KYZK8bfJ5RUGC/pnlXoRa2Q/JEuitpHQKHZOcsRLtMHQrw\newMJxyb5o+Xiu++8C2/cXhfzrqYgimsDCWdNQRvtALkIZxQUs3eOd63Juy6tKUBozb8HIg0X75h0\nKUceeRUK5eksbcyv5DnmlN+ktbggL9INPm916SjGU/Tfmtouq1GHxEMdLPfNcrm9LtEppLBFzWMH\n686CPCnvk5dqRZLcdI7PG3RspO2jr/INJPUV5cn1W5SbsYSsb1WQUY2BHLcgD9G+2T1m4vCxjYSC\nd5McP/dR7tLG2r6inLOdBO0SUzDTpMZ4cW3K7+viN76wMV6aLsbtvMA87U3ebG6qZUpRFEVRFGUP\ndDOlKIqiKIqyB3cq861smP7ma5zK9+cwp7YtmF83a5hoex4+X1swM44eU36nAUyRFQVuy1OYMVce\nTIYumfyliTLMNjAT38QwPbZqvEtEJKAAlc8o/521wX0PKKJh+xzmzegS7zM12iVoo0t6Lsl2c5g0\nvS7uf0zBLU8z8tY5ggxh/MN3c9OD/PHB+Q9trx9cwJT6nCSM+hn0mZFLnjHk2bH5FfL2HOLz+YRy\nRJ3AlGyZi+11lcO0u7mFuf2mSV4oZBbOrF1ZaPMM0u51F2PtvRRS621Jnooffmd7fUSBN1sDDvSH\n+och+jKqUM/sCXmXNtBGxZhyEJJXZF0dXhYSEbFrCrY5x/jyu2jLjXu2va5KmMnX5H0VWqhnGNB8\nH+PaIom/2cD9xTlkpXfa6KuBQRs5S/Sbl0NiuCl282iNx5BlZhH6sNPH+JkMMP/rGfqw0cI7lpRX\n7uMVJNZTCuB7a1PwzzUFA7TIA7lC/SsaU6Fz+Nx8DRcy5TrGGmUL+i8YYL401yhD5qA/Bj7Gnd+h\n9XGAow/LW3gQj2dUlz6eX1WQ0ZY21km3Del7ScckvIK0XBFZkaxoFyjHsE+ezy7Jq2PySDPkIUZr\n5SghL0SX3scev+TJ6ZaU47PC+EjJw/Nh9/DBkUVEqhJjzdzDOO1T4NiS8vEtBGM/eo61zBmibxsp\nyvrR6JfwOcnrFUn83QjzPe2hLZZTSGSm4KDceO81eeKLiAw8/H6/20N9Pr7BHElo3bYLyk1Ia6fQ\ncYmYfkPb1D9LOl5QU7DgtoP1xaYjPqak3IfHb5bTVi1TiqIoiqIoe6CbKUVRFEVRlD24U5nPqWGW\nParIO4BMwkKmZW8C061Nnh8PjmCizSnA5jXl1ypDmAbDAOa9pg9T94rkr9UC5v+ach7dfwCTYZju\nemvMc5jTWx5y4Q0jNi1CVvQWCDJ4fE6eIhnkjc0UZS1cSHXdY5Qp/wimzj4Fu5tQTkGXfKC64Zvl\nGHodvBht0UogJWQL1DdfkfcE5SxLpjDb189gMo58mP1vN2iTWY428Uhq22wopxJJFXWJtl09gnn6\niPIedqpdecWkaKMHNC38BUlylOcs2JCEOcf9HklelEJRuiRZHzco59mQgr2WCM6XUp0zizzqvMMH\nYBURmZGUUgjmpjQo2Cp5p3kBeYalMIcvYgR0HG9wT9uClEKONJKR161FQU5v6Z957S7G+HENOeP5\nDUu4mPsiIsNTzDWHPHrGS9SzQbnUTBfj8PIGY7uOME5OWl/aXvsLkl5dBGQtbMrblkEKdyl3p03r\nYH1J+ToPhNcjbzgXY6r7HO37jIKLNmr0dxhi/l6usf50rkl2bmMOJte4f0kBNeMl2tByIce6zsPt\n9WiK9jnz0celvStlJw7kHF+wHkdtmi8VfkPmdOTCHuAoQEF5I4cu5pTXRV8uyMuPA/s2PLRpTMFl\nbUE7VjbWh0MyOEFZgxxzcEL56CY5BUO9xRys6MhKSMdAeg8xrivy8gtJInTpeMFshrpVKcZCl9b1\ngIJPuxQV/LpxAAAgAElEQVTMtffKWjtfoo2fk4xuaE3tWhh7cfUYXyaP+gV5hScO5MamTfuGoEv3\n0O89eWAf9TD2MpoL05vdIN2fhVqmFEVRFEVR9kA3U4qiKIqiKHtwpzJfuwvppt5cbq+b5GGXCsxy\ntUOeCFTSgoJfFhSILahg3ssp946XkFeRBZmgSt/dXrshzMQdA5Np5wgmyvGU9AkRGdT4//tt8jiK\nyVQ4QT2jKQXzfBf17AnV54o8Gp6T2Xjx4fYyD1G3KoVZdunCTHq0Iqki2C33IUjJG24Ci6lMnuB/\nxhQ8rUkyx0VMZc4hnVjvQJo5p/yDf3sBc35EpuoNybTrOcrTzilwaAkp8DSHOdfvk5QlIpwOLF+i\nz5JHKMc5eTT1uuTBl8PsfbTCPeWc+j6g+ymXXYs8P2cJCuGQZLthz9cGPKkOyTpGm3XbCCS6yCCT\nzMh7piAvzKSBORiTN8xtBvN8THW+/xBjP1+TJ+uU5jLJZTcfozx5BtP+5EOsG467++/CpoH0Jn3I\n8QXJWLkhrx8KXBmuSEp10N6hQ8E8GxR4kPL0JUcon1VhvbOf4f5BC/Piita+QzG9htxiGZKgT9Du\n4QRlWM4+2V5PnmDMPiOvxvfbJE32Ic04R2gTf0ZSII33PPgA1yXG+GaNObiggJdhuBtRt6KAnKMV\n2uuhYL2waPxWQ/xu9B5gHDnX8OCqKNhmTUFUowrvWtR4V3zyZbzrkmTHlMfd25HgrYjG0YoCo5Ik\n2WihbiFOS4h/id+QekjjlOTS7gXWY9ug/l4T9XH+H/TtKMW8vvcer2UYF/4abZe9ssu4KVFuv8K7\nTyus1WUP3oanFtr+SYygohHNwVsKtL10yQvvBHVuuhhvyQi/UyvyEG0YjAu7frMgrGqZUhRFURRF\n2QPdTCmKoiiKouzBncp8lkvB1/owrT2fwBR3kUHSyCLKw3QLM/ycYme6x5S3p6AcUz2Yilcx5ech\nL8J0iNP6NxPKBUb5ta4ekQdIsBvEa0EBPYMA+9JVgRxj/S7MoOEAJtToFubaggLreRXMskkGSfK8\nBRP9KQWH+2hBQenW9PwPYPb1bciCh+KMzKeuR6bhBdq9iMjbYgqJLaRcjDblXzy9ovxXLszzpYth\n+uibJLv00eZHpAOfRrBzZxnKtinw3tPFrvfXfEp553YCY2KwHZPX4lGXcjnm5LVGkmedYfw+6ECq\nPKsoCGmEMdUpYT6fbPBdSVA2u347UoKTw8SeGIyXwmZvJZjxa5J9cgtzx6Gcij0bz6xOIGVfb9D2\nAcXgSymg4+kC8yO+gmdjTh5/pWDur0e7clnShneuzPHukqQriTE2KhvzZTJAJ170HuIeckoajShg\nZIn+bK5RN6+HtaZxj+agR17NHtroUIyusYYYLD9SWTQXMtxTUlDUDXm85TkFJab8hu4V1s2FTUGT\nbcy7MkRfFhXatnlEefAo+KfjYz6ti911djPFb0LYoCCqCTyoY5LtmuQt2ciov+kYwWoC6fjRx2iL\nIkEnG5KvW+TZNeyjrCvyfB3lh8+BKiJyuXqyvbbIUbPIMKYskqdaXZqbC/TJxMEciTzM2fEUfT4I\n0eeGPHyXEWThQYL5OLzE2GfpdLVEgNtG4/2d+nzZoH/LFcbkYwoS+r5g/icnWPMWCXlVUm7cRUb9\nRvkVG3QMxrVpbFPe1IRkR7eF35TuA83NpyiKoiiKcmfoZkpRFEVRFGUP7ljmw+vKHObXoxZMpWYF\n02Izgo3apsCF1ySBlOQ555Nkkl3B1LeMYK4bhhT0jJQen6zB4wxmVQ7IOLy/6wEW+pAPVhSs0l/C\n/DjP4B2TGs4HRybHAnWLR9jfWgXqOavxzKFD5uceeer5MGM3yWOuyt8sx9DrYPnkSUH5n8IBzKfO\nM5TNIu+vtoPPA5vkwgr33FCQS2uBd3kCyaCbUb47CtTpkhl+uoEsFC9gzu9lu3m0oibM4e/baOuR\nwLx9FkBikBQSRUrB/c67FHiu87XtdTPE5+9QTsdRgcCFmz68nhLKUTn5NiSGWQ7z+SEJ+mhjy0fb\nNymf2fMWeTpdkscYBT0kNVrqE8gQkw0F/KRpUI7RviWNkaWL9p2RN6q1QVv3LdyzDkj/EJF4g+8U\nFiQDWnbEG8JTsdGk3GMk9cxzCvRXod/mJA3k9O4h5aGzZqhPTjn4AvJ0s1uHn5tZTh7BFNjUDiDt\nBE2srS55v5o25kVcoo8HhuoSUA7QNtacdAk5ekFerV4HMp1DEmecYj7la8xNO0N5RERaJ5ib7gDr\n2vg5BSlOsS5ICU9Oq4k+cyM898kUHnINA3n9htb0zpqCkNJ8vOIYyDF5SJrdch8KjyTvnPK11i3M\n04Lyxno12ihu4vo9GqcVrWsVBVu9fIb2Chw8v5OTZ32OfpvT/VYHfXhvifFlKH+fiEjWwppyTccw\n2jRPlzR3pIkjEm0b5XZK8raj38EiRvmmGe43M4zJZYh117gP8ExanPzFmx2pUMuUoiiKoijKHuhm\nSlEURVEUZQ/uVOazKV9eL4LpzrJgZuy2YA7PN5Q76wRmWUlhToxJkskMeVi9g++6ZA68KWGK3qxg\n0j6hk/udj2GKvqlheuxSkDgRkQv6XzvEsyYeBRZc4rleTSbnPsyJ649Q55WBp8QPkyRpF3jm9dOv\nb69TA+kxO4ap8xF5n9QtkqcOxE0Mb5gmlSH9iGRKAxPruwV5/B2jL5s+vrukwH32Au120cP1yYpM\nzEsKqtfCeDpdogxPXPSL26UOi18JfhnAvO8V6P933IA+h2k4cmBu71G5PcrHljxFTrLChzZgt0nC\nbEKeKKjvNxuUuzhCnYP08HkWRUTiAktBx5BMJJTzkPLuhS14y90m5DHTxRhPatRh0MJYNoY8FSmA\n5fwppIoJBXnNScHLckg4pUXehRXaTkTET1GONER7Lyya8xWki6JG8MnSQ/mejiHTF22apyRJikXy\nJMnTcoIxnzyF55lN+chSyhF2KLIZ1tNVjeujL6GtFwXKXFDuO5dy2U3XaNOn5Mk6oPnbcNGGiY8g\nyGEDbbhe0ngaU644ChB5YyjwrUdasYjczNDnVYJx1GlDSnr6+Ft4n00BQ9eUy488GDsr1H9KHnlN\nD2PQ72Nu1pSLtKCjH3aDctAd3jHzBSEeHFNQUaeNd2ckw97MaG2mIwV2E3Xok8f6k4py3dqQPI9J\nUrQHFKT265RPr4s1uD1BHxbkXdcJSYIVEXeGZx3T0Rfbxxz0yHN4PaXg1TM6gkCBk4MGnhPTb8T9\nJsbSbQPf7VX4/GxBMjfl+xuN3mxuqmVKURRFURRlD3QzpSiKoiiKsgd3KvOZNQV7W+LaacDMtlrB\nJGwSmNxaJ7j+zQFM9b/cgNlzDXVGcsrnZMgDIBqTCXz87e11t/WV7fVVC81S3MA8vx7t5upZUqDD\nsxBmwxMKaNjLIR+lGeUbm8E8agSSU5c8EaJjCsrm4jmbG5gfc9JAOgnqNo1gfm9TPsJDEVEeqpKC\nl0Yh5LZBG5KBoWCOYY7vLingpUem2oFHMu2C2oE8fRZjSLBHEczWQQ3vn/OHaPPeiN41vL9Tn0nM\nAQQp7yDVszmHHNAlrzqvxrurOcqUrUjColyU3vvkRZaiX5ce+j6r8XlgYWw53YfyNjAxZJnGALLq\noqB8gRQUdnmCfrAyCrbq49ohj7du72x7XVNesM0tZFH/PubTdAS5JaVcaBy01DQx9qN8tz9ndFwg\ns/G+1gbXixR1TpfwOJuSh6kEuD+lHJGDFt7dbUD2yn2sa/ktnt/10J9JjyTI28MHYa1J8rYr5DKr\nRlg3nQGtuQXKUDjov3SNPsgoumpzjXsmAa4NSYRFDcnHopyOtoM1dEl5+hwXz7Far+QSXVBwVcGY\nMmuMKbuFPvApfqsteO6aoj1XLsq0Ie/KdgNzvKjR9+Ut3rtp4rsnOaTG8Zyiuh6QjYfy+QneXa7Q\nxs0G2nXmohxXBmOtuEL9p/TTX99gnXIp8KgfYu3Mr2keeJBd7TH6phVj3rSHdDwiJFlfRBI6hpFT\n7sgGBdssOY/eBmtEe4U1YjlAn7Robm4sDIB6g3ZpdzCG7QYkaW+INo1LtN3m+Zt52qplSlEURVEU\nZQ90M6UoiqIoirIHdyrz3Tz/le31uoQJsbMmmYssvPYKksmtD5PmWiiY5QhfyO7BNFjmkIbYm63T\nh0dXTh4Q1QYm0GQGs+IJBWTMEkgBIiIfT1GO4oSCdpLZeDal4H4ZSUZdmE2zDN4OZ6fkKdKFyXU2\np8CYJ7jfIQ9Jy4OJcrg5315XyStm8wMQXUAyuJ7CTOqRZ1udwWw7paCFjSOUzfsWTK/rDUy1/XdR\nd6uAGdoeoR3evSBpwIJkkBnqy+9gbI0GeGZ3tpt3KR6RV+iQA4xSQFULJvNeRrLiDcZLVmFMhF2S\nDO6jn+ISpupFiefPSIJOSOK+tPCurz5AnQ9JRMEUsxWuHw4xTldN9GHTwVhOW2i7xMLnswrl3pCT\nW0yyoH2Osbz5JiR+a4VccL5NQf/6GF/lmjwbH+563vgdfP+iwN9uR2jjiUXyJHn89SysHRXJhQEH\nScwoHx8FoV2QVOGtqHwZ1p14gqDAcUwNcyBc8iJsJZChJpRDsU8yV9HF/BpNIEfaFsoWT8il8iHq\nchqiv8cklVbkITidYq1o9dD+/QeQBasV58rc9cxsd97ZXpc2BZ6ktSZaYa65IY5m3OaQ/DtD9Efx\nIfp1sMZvi0se4cEQ4+DpEGOzSZ5gI/LW9ltou0NSUC5LcwJpc53hfVZCsloDZe3QCY+Mcg36l3hm\nLGjHIXnjtkr01XKFd7kd9GeTcpyyyFnTsY402HVz9CifrEV5RzfU9u0Y6+KyS56UHjz7mj76Z2Pj\n7VGG746bJDFm9PtS4/d642NtXlHO3PkzlO11UMuUoiiKoijKHuhmSlEURVEUZQ90M6UoiqIoirIH\nd3pmKjE4KxMViLTqDOhsCR0Jed6iCMrkQj6hSKvXOVzO3V/B+QCfvrtOoMXOPJyhsdeUkDiBTp+G\neFdUQpedJrvN5VKU31uK8B1OoBEXNjT1o1OUr2OjouUJItY+uoQmHE5wliF2oXcPMor8fI39cEIR\n1ssORfK1D+9+HdB5GHGgP48eUSLKE7zXLaChf/3JL2+vLyro2N1T3G/nOPNS0nmxDynURLOJsxsJ\nuUB7q7+7vV6UOP+TfAPjZmpz0mkRIY3eKdFPG3LrbSSo5+US/WTVeMeKzth0NrinpISe0xzu8/OP\ncC7Ffw/vLSg8h5PjzEBaUyaAA1L4GDvhAOVIKcNAr4V6OiHuv0wxFhYUxT30KEr6Y5yTGKU4l3As\n6H+hdpxSOJPAwTmZPq0h533cf32Jc0giIkMH53eKY5zNySnSezvG3ImneEdyjvZ2PMy7ps3JnVHn\nmJKKd5d0doUOgE4nOMczoCTAXffwZ+BMhnZpUhLXmsbdlPqgoISuPQvj11CojvKrtG7GuCe8wFhZ\nfeM7eC8+lg680KV0cUbOX6JNLAofnjzfPePZoATujQsaI8+wbm4KnBOqKUzNEWW2dlKs5bGhiN4P\ncH+rhXKMnqDOXoDvrnPMWc/G9dVy9xzmoVjMKYTEGdag8wZFca8xNqsYYzw9xpmx4pYSptdo434b\nbdTrIAr5ak5zgiLDD2boXJvmZmRhzmYUJX7Y2O3PaQPtXVFmD6tC30ZdXL9vYc3/ehu/Cy6ti81T\ntNFkhr56SGtKSmeKN3RGOjV4/ljQ/61II6AriqIoiqLcGbqZUhRFURRF2YM7lfmiFDJL1oIprrcm\nN/ghTIhhDJNm7sKMmfdI6ljAjDepYZZu9GH2i2pcGzIZluQS6ZAJMOSIzuRO3DnZ3Xv6JB9OyD06\nsiDvsPtuTAle7SWkzbyCKXZlwx01cCiCMplANyW5X5+Re+xzSg6b4v75W8iNaznop0GfTPSCdijo\nxYtLSoCbon39I5hbewZy3owi6z5bQ8qNE9Sx6UGeODqH+ffbN2jDMsXzXQpN4ZzvujFTDlCJKEJ1\naFMS1AtIXhTMX5qCcZ1kkLb4fdMPKSwISZJsVg6e4b2dAergdzB+i8FDeRs4BeZgPkE/ePfRDysK\nJTCjyMKbEPdbcwrVEWC+ZBS5urzCuJ74kAbaFkUqD1Fnu8K7HjQhC+QUhkFau+7XKXnaJ5d4nxuQ\nxBBBckpJonDHiPDsRZibEuHoQOZR4m6S4xckQ38wQP1LiqRvRVS31eFlvvWExougbKuMor8XFCH+\nCI112oXsulhQ5OkCdfdpGVwhEYA0C8yD0SXJJWeYm30K82EcTmxOoQcCkn5FpKhR1vU1hTCJMV82\nMep8RsOiPcR4rCq0dX6G3x83RPlskrOEjk34K9wfUngGoUwT5WQ3QfOhsA3qFowoiXMPfdKg0DCF\nA/mzXqAOTbKdTH2WLSm8QUmR4Tf4bkAJnZMl5lOzRtsZ+q3MWpTcOt89ZmJRdouwg7a0G5Dw3Q6e\nlWQU0iOg3/IQa2S1QbucUiT262M6OuLinnhNa0JESd4bmL/VKwmaPwu1TCmKoiiKouyBbqYURVEU\nRVH24E5lvoySwLbo80WfzcAwM3YNzMZOAya3muQ2/xKm1Q15fSUF7jnpUzLFFO+6CmCKnMxg0oxJ\nUgtmFKF1QiZgEclJorKfoHyXA/IGJJUgSHBPbCAFNhyYXDtN1KdRUpRp8lYIZyhfMcJzDHktcccG\n5eH3zJ2IPH3aKJtlI1pxRn3wLf/D7XVEiYGHEep4G8J7xmzY24gSa/po22yO59gRJIxjF9+9XkEq\nbn6F5OEN5A8REVPC7JtV+H6TTMBhCs+o5QIJZC8Tiobew3fHS4ydKx+yUK8FKTAir85VAxLWLEFb\nfOBA9jgevJ1kqiW15Zo8GDNKYh2Qd56fUUJnkrvXFfqqmKAOWYVyn70LjyG7gCQRbDBe5ktMnOYK\nnoApRZXP6RhAK9qVhqIQ71sKeUPRUQOTQ4b2Mry7PoV84NARhDyn6PYbiqYdof4dSp68usS7bMoA\nkM8whvMVSUYHIjXom9GCxnkD9WpHJF8L+jUw5HnVoATjE0o8/g4lwE2whrrul7bXUQWvzgElly4d\nrBu2A/l+NcJzTiK8V0RkZVG07hJzJGOvSPLePiWv06iCdLSgdulx8uQS716QtOXRc3If/efT2vGE\n7i+EPJwPSC+i37sGypHQmpXT0YmMpMpjH+31eI7f1s6S5LyCskTEkGdXlKXk1CUpn7IzuH301c1z\njOtVinHtH7+SbcJgPNjk5TyM8NzbNZ41X5JUSR6yqY3fU9uledTGd7s5Pp81KWNGiDIMXYydBa1T\n07ZGQFcURVEURbkzdDOlKIqiKIqyB3cq81k+zPJ1A6a184qCg1FCzSvy9InIS+h9HyZ9pwuZ6B4l\n71zbbN5ENY9PSIITmAmXOeSG2Q3JMxlM491iN/hYfQuJTSgp6ElKJvEnKN+ySZ535BJzRspN3YT5\ncXZFiWK/A/mk5cAU2yCPpLmF8tk52nqxPLzHUBVCJnhKAS83V2jH9ZTMuZRkk6343xqjnJWDuiQZ\n2japKXFn6+H2Ok5wT5zDlJw20W4Vho3US5j5rdWuZJvNEUAwv8CX3I9xX52ROZwSeS5cBIiMblDP\nFckbzYCkji4lLiUvFIo7KK026tB7D/LkKXnwHJK8j/L5JcaRk3FwO5R7GuKeC0pEfeTD8+bJBHV4\nb46xP/UxRprkgWtHkIbOSV4dUSLlyTXcKJ0U64CxMY5ERGZD/K3yUKaHR1/bXlseBWQtUY50iXpm\n9yhw34g8GJsoU26jz9sTtEtMHonOEp2b15j7S48y0R6I+TN6ZoO8XwOUs2cw10JaZz0KclhZFNR0\ngPXNBFjTjslTb/IMQTuLNZ7/eIH5cUGeqS3qv75B+0/NrvuxtYFX4SomaX9DAXx7KPdljO+fWFjL\nb55j3EVdWqPJOy20yRtZsFB5JMFnGRZsh+wRy+DwSatFRLo25nxNa7xDnm1CQa3rGuO0G6GsNslW\nUwfzqDmiIJwd9Nu5h/duppR8eoA5MaPE7gvyMu9jesijp/Q7KSINSpTcpd+77yT4fC6YL5sZyhqS\ntCdUz8YpxsWKjuyUEcncK6yjTToWErYwXmYz1GdAnvivg1qmFEVRFEVR9kA3U4qiKIqiKHtwpzKf\nE8H2d0RBLjdk+r1Occ9JCM+w6BxS1aaA+c0lU71Tw3PDI5motUbertEIniseUj6Jl8Fc/V4bJueP\nOKBZZ/d0f5DDDJyQubOVo6zpOzCzejOYIjMX8uE0gInaGJiK7S7MlW2S9r70lQ9QCJJF2+RZ8uHH\nMPXXZleePAQpBXBMKADaEwtSmk3B95oV+W8mML2GZ+j73CNpj/q4bKA/cvI06x9BbrieQo6qLAqm\nmqD/NtS2R+5um5DKI06NvolIPqjIC9E2qH+PZOQJebH0KJDoYo7n3JYwyXsezNk98qSpO5TfMaDA\nnkfw2DwkboY6zD202cA6217nJPk5NUkgQ9TN3pDXYgdtZJM0QqnvJHTQh98kCe9dWpoGTbTjaAo5\nr23gDdYN4GkpIuLWDl2Td66NseEJ+nleoBzNdzAmS8rlZ4bo2w0FN6wtjMlL8ki6V5BnLnmk5SHk\npmJxeO/MykddQvLaG9KYr+hoxbMNyty0sYbYLsZaRlJdn2T0NeWojFLKy0n50Y46uJ6PyWuS2tCl\nXJT33F3JllLwiUNeaOUJ1pdhG89dT9EfMTl59dvv4fNb1NOQFBSR/NlqY3xsJnRkgZ7ppLj//bf0\nc5pW+P1q0G/c9IbkM1pTmhTM1lDA45Tk+w3No7MhxqlF3ufhDOOiHuIPyxLz9PIWjRHWAd2PObeu\nd202RUWBVykAt6HfcqFcrtHpl7fX0xi/wbePcDTjLINU1yHZTmrKhxuTNE+5azeXGGCG2m7j7np8\nfxZqmVIURVEURdkD3UwpiqIoiqLswZ3KfDfPYTbtfEC2UpKhGmSGj7owAR7XMJN/OH+8vU5GJKn1\nIAdcxjCfBwHMzKYg8zyVpzXAdy0K5ndh4Tkp5Q4TEVnE2IvalJeoiGFaTy/hhZewlxAFKzPkzteo\nSYZc457wPuSWFXkcFOQltUgg7XUoGGSV7uahOwSnvwFm369/AybjMeXUuyCPjPAE/V2vYQK2WjCr\nrgSf1yQZTALKl7SBmdcK0CbWGYZyTQHZ6ifoo/EVSWrnu3KZ1YfO17ExRhIXJu26wBhZPfvm9npu\n4x15CxLRrIQX0uYU9fRLjOuKvFnqDpmbSf6yfZbgdgPgHYoZeWR2AujfFcmzbkayGkl4bgo5IClh\nVu+uKBcnebLmBvOR1Fl5aEHO7LTQ1r0J2qLq4V3VCGOELPUiItLAUBKvhzIVU3y/de/+9noYos/n\nKc1fet8pBadtjfH5OsYYyTKSgikvXk06cjVB+y5m35ZDk5EsHNG/l7MVBbyNsMYdk7fjosRcDkny\ncyn326PHqG+DxqPTJFmwhefY5IHWpvyAfoAxFCRYAycUNPnFfRRsknKuljeU01XwvjKlvKzkzZYv\naC630MfxDQVHfoj2ahmSeCM6ovEY5SkM2mi1fjtzs06xbnmUizaoUefCQlmdPo5aTNe0No0hC540\naa4F+G5AAYKLcwp8SwF4n5Is3OnQutYiT0g60jIudo/HDCjgc+OMj0KgfDM6dhEl5Hnn4bmNU6wj\nvo92SWPaT1DA0Jxy9lU38E6MPIzb1Qa/17LRoJ2KoiiKoih3hm6mFEVRFEVR9uBOZb6TNskVM5gi\nj0iey22S5NYwJ9542PclFsz2MXlcGPIQbFgkJbkwq8ckN0iBIGEF5QKyjmBjNB0KEFpS4DIR6ZLE\n2DuhoHM35NF2DTOo04O5sufBzDo0JGNk0CvSBnkkJTDpzkgubCQwYxY5vStEfQwFUDsUXgmJLej8\nyva6HpFsdUSejBb6dRaQx9sUdUxP0d855eBbPULb3i4hnZQxZM0P7kMeLSgn4LJPAfnOKc8eyUgi\nIlYNc31CuZ2yRxhTSxvm9udNyCTVBibwKXmjtkfoD2miz5wunn+1xD0RBbAcvIOyrifw5smv3k7+\nryQmCd6Bub1sUO61JvqzRUFxkxnqH5BclhSYg0PKR1hmkD8/usa4Ph5A/swy9EdKXqrSgDxzn7wr\nV/FuoMcleTfJBnXwHbyjDjFPHYN5tKF8nMMZrTVLzE2fU9gtMJ7vk2ztkpfnvIDnkVQUbDA6fBBW\nn+Z7JHScYEDvMuRtlZHHMgUgrWPMnQ09J2qRNyqts60K94wqjKcgwdzPZnTMggpaPUC/kkPVi/eR\nV2TLoO3ye3jfYoF3VNm38A7yop3RsYkTClLrfY1k4DnWlAXJdo4hSZG8xdiD+ol3eK9pEZFqjf65\nyrDWBnSUoz7GmjedoF2PK/x+ZRY0ddOjAMkOSXJPPt5eR2fwHHTJ7BIGWAdKB+uxqfH54jG8P+tX\nmmXq0fcXODpQ0nGcgLyCCxvvSJr4/MjBeG7llCOxRv0b5KXvRFjXxhP8/pYN/K7na8rBmLyZp61a\nphRFURRFUfZAN1OKoiiKoih7cKcyX5LBXBecQCbK1pA3sg5M71lMgTonMNe6NczwPnnY2V3Iec/m\nMGkWCQUA9GG6uxxBbkgbeM6AvPEGLky9k9WulJDbKOtojPdZFeV9Ooe80aC96wnJFTnllZuP4HHS\n76PO5Qpm82QDaeTUIS+0CPfMUphDveHhu/m0i4CqRz1cv3sEM/nVR5A2vAcwVU9wKTPyvGs9ocCG\npE0uc7QDe0s1A8hLqQspIFvAbDsTSBU+BXD7UKgQIlLaGAvumDyAliQ3NjB+0xh91qEAdWdjCvT3\nQ+jviHI3jn0K/niJsbyg3F7HFOTSOobukXiH98wUEYkWGMshedGuKNfcIEB9HAftWlEurDxHHZYJ\n5u+5DbP/grxqGhFkG4vy+nlTmORvKeVZg3J05hRg0X4lMO0JeWc6GwrCStLe8hnGT0ZSlLVG/8Tn\nGNcb/fUAACAASURBVMNpgT6vSKqYp+iflofvDoYkQ34b613pYY5b5eGDdroURNcuyYP4Fv06qTDu\nIhdt3WngnjXJo0Py+LI2qKNNRyuWS6xL3RPU3aN0anGXpHYKcOs20V8nhjy9RaTyML7GNC7aNtr9\n6VO822minwIK3tzrQPJxPXgjF7dYa8bkweW00GfRDUmKBvfUFMg2me16ex8KhwLQ1i6NqYqOVFDu\nx/YJ1kKH8toJzdk6gvwXkzfu3EHbJTHmSiPEb599REGXaS57a5Qh6GFcx/ReERGLjkI4Psrao7yA\nlynyPF6898Pb645BnX9pDa9+pw8JvpqShz9tcTZz1G1BAUzLKdmUKKC0E7+iN38GaplSFEVRFEXZ\nA91MKYqiKIqi7MHd5uYjiSKc4Dr6DTCV9lsw9d9MSDqrYX6zfZg6azYZljDnnzdgZlw3YX6sSbYI\nApj9QooTN3RJbnLJ5Ew54kREghQeAf4TmFOXFeUJGsDEGbUgJaTkQRNTQL9lF/e3K5KbWiQftCBX\nsZk5p5xULcrbdFMf3p1vXZEXoQOvj2UMOWeUwhyejGCq95rYw/cNzLzFKdrz8sOn22ubcnV1H+Jd\nOcnDC5ICxwvU94oCfnoZymMEpmARkWSJ8lE8P7l1SSYkDzP7mLzTcvLy89Ef7wr6bER5HLOnKMdk\nivYanCOIZLGBp8owgvTSl90cdIdisqZcWGTqb9v4/PIW9WxSjjSzQX/OSIXsk/fQ1TP0j78iL1iS\nevIYpvqYAmeeleT91oUkYTL0/4bkOxGRSDBOegPMebuFciQxCjt10Q+tI7qf3t1ekbfwEHUuLZTD\nS1DnmxRju84RnNM3uD/ySMM8ELWDfpovKODhAPJ3y8UalRYkeaxpHsxIOmtj/EaU48928a5Fgrp4\nS5LUaK4FPgU6tiG1rVK0WzqhnHMisqS8kfeOyDuNbAH9OXnL+pibbQoqWU1oLnfQ3+kK886fkZdX\nQsFYC0hQWYVn3s5Qt26yG2z0UIwcmps52smLyBtV6LiEg3LEHuXHdEhSfoY1yO6RB/IZjsokDQpE\nnUES753g+RvyHLR6mLP9Nuaye7v7uzk/wphpNfGsVRtl7a8xfsqaZGvKI/ilE4zhYIrfxKyD8dmi\ntnvuYfyzJFm6GC92A2N4Q7k7Xwe1TCmKoiiKouyBbqYURVEURVH24E5lvmJFXm7HMA/6DZgu5yuY\nUNcCM/y6ghzQ9WC6q/p4pl+RSfOYPE42MNE6J5STaAGTttMkD4UKZt/RDcybUu3KZaGF/88uYCpM\nZ3jWd6Z4R7RBPbtteIo4Ag+43jPymvkyzLIn5H1QkNnz6QTPb5Dn2TJBuZfTXc+1Q5A8hcm030Z/\nHPkUxI6CZ95+RHndTmGSbQwpaCVJTc0BzM15gbosPPLMJI+tjII2hmcwHX9QUm65EBJBHu3KfBkF\n3LsmE7DXganXC/C+e23y4GpBdh5sSPIcob/DBIE30w7leCNvKIp/KBmGqfgR3jVpvZ38Xw0HZnkr\ngayY1xSEMiZp+pQCcg5J1s4gDYwNPp/fYE5EFIQvsCkXGL2rtUC7zBPM5YIcg0yOsXAW4DkiItEC\n7XQzQ93uZ5jba3pYSLKiTdLuypBcYZHH6IiCYZI8uWJPrxxz5KMYa8XJhj1MDy/zPZnhmedNjPMj\ng0FV15hryznmmhOi/PMYz2m4JJGT1xU5gUpFc7+m58wK9GtNxxs65LGbG8yzVbUrr/RIznlwin5e\n5LhvuSDJlzywyznqWQrWU3tCORQjlDU8wvgQmmrLFO1YztCvhurgfZUm7QFxj8hjzpB35nOMI+8c\nfTIgqXlGslVVUuBkype3usX8eqeN3HRVSh7F1D+nJOddxfjuIkF5LAdr/2qA+S4iklLQ3iLCHG6Q\n3DY5ooCxMeVdXKF8zgZ9m3ZQz+YcfZK67+K7Hp6ZHuO99opyHJIXvEkhBb8OaplSFEVRFEXZA91M\nKYqiKIqi7MGdynw+eQck5B0yG8Psl1BwQ3sOE2LzmDz4qNi+DZPjlKzD9YhkuyGCSlYZBUb0YT5e\nUG4u16WgghRU8upm18tk2If5ukd59GrKK1RSkFCbgqOFOeqTl3hOEMFEaZP32DSmZ7KcSd4aa/Ie\nc30KjtdEWx+Kawtt0fBQ/kEICa/dhXzZjtDW6wrm8KCP8vstyA09ymkYd3H/6hO0ybMdiZcChC6o\nvyt45DgJSXZteBKJiGQU6DAb41nOEP3vk0yQkvdYsaKAjCRNZwHM7R3KwZa4KEdBwVtNgncN0RSS\n5uSBungzD5PXxfUhE8QRxlE7Rh3qh+QZtYIUaifokzXldrPmaCM3Q7mbJJf1SCJdXKIPFiXlSiSv\n3vUVxnWvTXLsD1GDya5HWESp/VYp+talIKT3T+E9lMQYM9M1+qfFHqA+ZKWyRH0WNiRpi/LQtWJI\nQ0/GGCPH7cNLQy1D60OANXdBnmANiqTZa1Mg1A1kpOMW2jpnD8ccc7yykAeP0qeKm+H5rQjfXZOU\nvyZpPU4wDvrebnBkY7B+fePblKN0iqMD85ICdVLdSgufW3RE4PoK0mxCa3HrIdouocCsNeUyHDZZ\nmqK8fsHbmZsBeRXma4ypxQp9Faaowyd0ZKW9wLrTCSDtrR1am0L0/ypHJ6YxfpeFPLanLr577wT9\n+Z3v0BzvoDzpko7KiMh0TEFCKcCmOad1p8bnGclt8RhzXiyUqbnGGFlQXy03COzp0/GNE5eOTgjG\nlEX5USv3zbzg1TKlKIqiKIqyB7qZUhRFURRF2YM7lfnmE5j7vADm7WVJklQNU1zahInaJs8Fvw0z\n8CqHlFBNKVhdQIHLriE3lTHuudeH50abvA+KDOZjiwJhvtukaI4iklNOLjsizwfKE9d1IR8satR/\nfPMI5SavtLqGebQkT70lBRBrFZR7boP7DXk9xORA0Q1xz6GYrlC20IZ5NiLJ4IP3YQJ/PIb0lpco\nnDGQyFzOQUVeMuunMMkvKM9a+pgCdZK8Fm3+zva6PMH9bgaT92C9m/+rdGhsttCXmUtBBik/WVli\nDHYjmLoTnzymKHfW0qYcdIL6nLgod9Cme+j+qIF3bTL0/SGxaD52ycs1CPDupkWyj4PyOQvIKusJ\n2rvhYL40yAvJs9DWmwrm+Yo8dtszjNkoIkl8CHn2Hnlkls93PW/smrxfSd6KlpQT06KguNeYOylJ\nRu4S13ETdWunGMPLDAFmBz55IVJAQpOjvb4KdUKazm5OwUMQ0DGARu9ie83BWKNT1CWbo+7eGvd4\nbcyXmBJqZhHGb69D8nVFwRhpTYtD9viC1JQvMJ7WFgU37sOrS0TEvqKco8kn2+sRz2Eap0clPs/I\nkzDOsa5HJzQOKNeg5Ch3mOO7YUgBaDOsZb7BfLycvx2Zr9xgvsQx1iOW0rICsvZRSskQfawvNh1l\nue9gjKzexTi1KKBuEuA5IUm1vQht16S17ysRynZzg/48rXcl+M5XMReWlEN3QHk9W22MgSRH/Qc0\nnmvKh+uSB2hI7dIk79+nOdoi9jBWC8oLWVTkXTt5szyoaplSFEVRFEXZA91MKYqiKIqi7MGdynwr\n8jLpTmEaNx5MeqFPAd58mOLGJGclJUyaDskHE5LaWnFF91BuIB+fWxT0j+WJWQnTreuRt8YaJmoR\nkYrysM2v8dwqhAQwo6/czykY4oByY+UwlfshlbtFJvFLmBxz8kg0McleJG22KS/aot71jjkE4zXq\n2FtRvjMLfRYX6I97DQqk59/DgzrYz8cV+riq8V0nhLm5DwuxTCz0fbsgT1EP5Qlv0T7WBeWB243Z\nKS2bTPoXkJJ8kldDMpM7NvrjpoLEVD6DSbv2SFbxYW8+Jm+gso3x5QnGQZ9M6RkFLY0Xu/LkoSjm\n5PVI8yIhacuhfHc3G5RvGGEsxD4kbt+CfORm6Odsg75qtdFGhjxnTUBenjRv/JRyPJJcbOeUd0xE\nntWoTzOlaKhNvIO97UYG/WMop6RQbkKTYB1ZOxSgMEUfuuTNFxvUP2hScF0X9Uniwwdh9d6DFOKS\n5GHPKQDrGO9d+vDasjOSTivyNCUPT8fBGPfJQ7bfx/ithby51uSZRYFcgxDtUwnKls7IY0tEnAB9\nu6jRl9WGcr3Sej8+R5/ZGeZmRvLXinJCUhdLucTYXzRp/l7ippsYx0bWG0iepXP4AKwiIieCcqfv\nUE45Qf+0yfvVGLR90MIxk5zmhNvGM4NrPDPy8MyJR1JwQTLnnHKL1nSs4Qb9ubYxP9zwlSMVHqT9\n/gDrRYc6gtRmuR+i/0PB2pF28Nx1jncvlqj/JKQjCxWkygnl5bRyrANHZF+yvrwrT34WaplSFEVR\nFEXZA91MKYqiKIqi7MGdynydIczkNuU5G1CALkOBz+IxefmR2d+rsQesxjABNn18t6RgakJyS5uC\nSopDksGaJMIC5r3TDsyQy8Zu8MuYPA+9GeXgS2DutsnU75AEZgzMlUMXZslxBBOyS56EfcoRN9tQ\n0EvyDBuwJ2AF867VO3w3T25R3zSDebYgSa5HEt6UpKOAAq0akvNOKefgdIDvtgV1cWyY+R/a5OVF\nEmx6gv7OCrT5aI7PQyqniEhGQdyCAu9rdFBPSzAWnDbMxC2SRiZHkHnOfeQRazgkYxTkbUXySSCU\nF4rGZj4mc3b5djyGiiVM3cUJxv+8QJtdP4M0H4UkBxi0ZUayR+xQ0LsKbde0UJ94Dam2tYRMsm5h\njOcUBPfRM7Rvq4m+/cAirUZEalLq5pTbbDOi3HkkK0sP7TpnL11BmabkwWdbmL/NDt49W+C7nM+s\nsSD5wIf0tJRdefIQbJYo5ynlO1uT9L+idg9jrGsrWkMWCY47NEhSalgkKZEUdk1yqkmQQ+3mBu/i\nsd89xzzzaU33nN2AugXl/OOjFnab+uYS16eUi9Tt4FlZlzx7yatzQgkGsw3lHK3fx/NTyr+YI/Cr\nkLdnunk7Enx0Qe8Yo53WNcZRQcdRWhRsMl2gPoGP8m1WuGdKp2DiAGtwvaB8miX6tqZcrOnyk+11\nntM9lA42Mbte8LWgTPEaczuv0c+xS4FKXfSt68ALvliSJ+yU+tCieU05NG06BjMg7/3gCGMyJ+9P\nf/xmOW3VMqUoiqIoirIHuplSFEVRFEXZgzuV+SIKoJUGMF1OE/Jso/w5CwO5obmCidIyFDzQxv0V\n5S1r9SCXTaeQGK5v4blSGZgArQLPHxxRoE3yEos3u8EvN5QXLyooQOMNzK8Dg+c+Js+wgiSdsEWe\nVB+ika4431QD5m2XJNIqQt3SEcz4VQzTqHO564V4CKZP0I7WEGbceYk2CkkyaOYU2HOB+pZdyByb\nNnnwVRQsk+ReaZB0RBJq+wweTNYG98fXeJdFkl3g7Pal3YI0G5WUm88hbxgf7176MIE3P8IY6Vgo\nN8X8ExMjYKCbQFZZUx6yMeWyMyRfC80V8Q/v/SUi4nYpyGKOsexRbrdKUL4uSQlXa4y1aI0xXvdI\nOqRAoJeUX2y++AT3ryGd9iO0kSOQS+sY82N9g3H9N+Nd+TPEV8TUNPbI8/BmQ2Z/8vjtZugTM8Dn\nFvXt1KE5RcEDox59t4T8eY44j7K4xtz3Frv5Pg/CFFLopPy722tKUSjphtauEwoESd5Zhsb+YoH7\nv0Fefg1a93zyWOwG722vHRtjn39yzArXBZUu9He9+VYl5sKgBenNNih3TUEYVyXKlK0pqPGQ8ruW\n6LNT+p25usA9Lrn8VjQPoksKUkvBPAuSwg5JTMFynTYGtheSV2yKdWEjGLMmJ6/NBuaIsTC/jIu2\n92kexE2M0xnVrUWekGtaa206mpC4uKe33pXgx+SRO1tADo4dvOOUgqomMfq/vCF5jvKJck7fBuXc\nnDaxplS0Bq0p6LSdoB1HNKbKBcm5r4FaphRFURRFUfZAN1OKoiiKoih7YOr67cgGiqIoiqIoXwTU\nMqUoiqIoirIHuplSFEVRFEXZA91MKYqiKIqi7IFuphRFURRFUfZAN1OKoiiKoih7oJspRVEURVGU\nPdDNlKIoiqIoyh7oZkpRFEVRFGUPdDOlKIqiKIqyB7qZUhRFURRF2QPdTCmKoiiKouyBbqYURVEU\nRVH2QDdTiqIoiqIoe6CbKUVRFEVRlD3QzZSiKIqiKMoe6GZKURRFURRlD3QzpSiKoiiKsge6mVIU\nRVEURdkD3UwpiqIoiqLsgW6mFEVRFEVR9kA3U4qiKIqiKHugmylFURRFUZQ90M2UoiiKoijKHuhm\nSlEURVEUZQ90M6UoiqIoirIHuplSFEVRFEXZA91MKYqiKIqi7IFuphRFURRFUfZAN1OKoiiKoih7\noJspRVEURVGUPdDNlKIoiqIoyh7oZkpRFEVRFGUPdDOlKIqiKIqyB7qZUhRFURRF2QPdTCmKoiiK\nouyBbqYURVEURVH2QDdTiqIoiqIoe6CbKUVRFEVRlD3QzZSiKIqiKMoe6GZKURRFURRlD3QzpSiK\noiiKsge6mVIURVEURdkD3UwpiqIoiqLsgW6mFEVRFEVR9kA3U4qiKIqiKHugmylFURRFUZQ90M2U\noiiKoijKHuhmSlEURVEUZQ90M6UoiqIoirIHuplSFEVRFEXZA91MKYqiKIqi7IFuphRFURRFUfZA\nN1OKoiiKoih7oJspRVEURVGUPdDNlKIoiqIoyh7oZkpRFEVRFGUPdDOlKIqiKIqyB7qZUhRFURRF\n2QPdTCmKoiiKouyBbqYURVEURVH2QDdTiqIoiqIoe6CbKUVRFEVRlD3QzZSiKIqiKMoe6GZKURRF\nURRlD3QzpSiKoiiKsge6mVIURVEURdkD3UwpiqIoiqLsgW6mFEVRFEVR9kA3U4qiKIqiKHugmylF\nURRFUZQ90M2UoiiKoijKHuhmSlEURVEUZQ90M6UoiqIoirIHuplSFEVRFEXZA91MKYqiKIqi7IFu\nphRFURRFUfZAN1PK/8/emwfbtm93XeM3+zlXv9buT3u7915eGkITAhZKCAgkVjQVJDYYCzRYpYkQ\nrZIYKmIshYhKk4gajVKWUAFihABlyqJSAQVLbBI0JHl5793m3HPO7tde/Vqzn9M/9n77+13H5N57\nstbZ98U3PlW37jxrzzXnr59zje9vjKEoiqIoygboy5SiKIqiKMoG6MuUoiiKoijKBujLlKIoiqIo\nygboy5SiKIqiKMoG6MuUoiiKoijKBujLlKIoiqIoygboy5SiKIqiKMoG6MuUoiiKoijKBujLlKIo\niqIoygboy5SiKIqiKMoG6MuUoiiKoijKBujLlKIoiqIoygboy5SiKIqiKMoG6MuUoiiKoijKBujL\nlKIoiqIoygboy5SiKIqiKMoG6MuUoiiKoijKBujLlKIoiqIoygboy5SiKIqiKMoG6MuUoiiKoijK\nBujLlKIoiqIoygboy5SiKIqiKMoG6MuUoiiKoijKBujLlKIoiqIoygboy5SiKIqiKMoG6MuUoiiK\noijKBujLlKIoiqIoygboy5SiKIqiKMoG6MuUoiiKoijKBujLlKIoiqIoygboy5SiKIqiKMoG6MuU\noiiKoijKBujLlKIoiqIoygboy5SiKIqiKMoG6MuUoiiKoijKBujLlKIoiqIoygboy5SiKIqiKMoG\n6MuUoiiKoijKBujLlKIoiqIoygboy5SiKIqiKMoG6MuUoiiKoijKBujLlKIoiqIoygboy5SiKIqi\nKMoG6MuUoiiKoijKBujLlKIoiqIoygboy5SiKIqiKMoG6MuUoiiKoijKBujL1C+BMea/Ncb8Bx93\nOZSXxxjzSWPM/22MmRtj/tDHXR7lo2GMeWKM+R0fdzmUu8UY833GmL/4AX//eWPM191hkZSPAWNM\nbYx58+MuxyY4H3cBFGXL/BER+dt1XX/1x10QRVE2o67rL/+4y6BcY4x5IiLfXtf1T37cZfliRC1T\nyv/feCQiP/9L/cEYY99xWZQ7xBijPw4V5WNA556+TImIiDHm1xpjfuZGGvorIhLQ3/6gMeZtY8zI\nGPM3jDFH9LffaYz5rDFmaoz5z40x/7Mx5ts/lkooYoz5KRH5bSLy54wxC2PMjxhj/gtjzE8YY5Yi\n8tuMMR1jzH9njLk0xrxvjPleY4x1833bGPOnjDFDY8x7xpjvvDE/f8kvFHfEVxtjfvZmPv0VY0wg\n8qFzsDbGfIcx5vMi8nlzzZ8xxlwYY2bGmH9ojPmKm3N9Y8x/Yox5aow5N8b8kDEm/Jjq+iWHMea7\njTHHN+vsZ40xv/3mT97NnJzfyHq/gb5zK//eSII/djM25jdr9q/5WCrzJYYx5i+IyEMR+Zs3a+sf\nuZl7/7Ix5qmI/JQx5uuMMc9f+B73n22M+aPGmHdu+u+njTEPfol7/RZjzLNfbfLul/zLlDHGE5Ef\nF5G/ICJ9EfnvReT33Pzt60Xk+0XkW0XkUETeF5G/fPO3HRH5MRH5HhEZiMhnReQfuePiK0Rd118v\nIn9XRL6zruumiGQi8s+LyB8XkZaI/D0R+U9FpCMir4vIbxWRf1FE/sDNJf6giHyDiHy1iPw6Efnm\nuyy/It8qIr9bRF4Tka8Skd//QXOQ+GYR+VoR+bSI/E4R+cdE5BNy3c/fKiJXN+f9hzeff7WIvCki\n90Tkj7266ihfwBjzSRH5ThH5mrquWyLyu0Tkyc2f/0m57tOuiPwNEflzH3Cpf0qu1+i+iPyIiPy4\nMcZ9RcVWbqjr+ttE5KmIfNPN2vqjN3/6rSLyZXLdnx/Gvyki/5yIfKOItEXkXxKRFZ9gjPndIvKX\nROT31HX9d7ZS+DviS/5lSkR+k4i4IvJn67rO67r+MRH5P2/+9vtE5M/Xdf0zdV2ncv3i9JuNMY/l\nekD8fF3Xf7Wu60JEflBEzu689MqH8dfruv5f67quRCQXkX9WRL6nrut5XddPRORPici33Zz7rSLy\nA3VdP6/reizXD1/l7vjBuq5P6roeicjflOuXng+ag1/g++u6HtV1Hct1H7dE5FMiYuq6/kxd16fG\nGCMi/4qI/Bs3585F5E/I9XhQXj2liPgi8mljjFvX9ZO6rt+5+dvfq+v6J+q6LuX6R+0HWZt+uq7r\nH6vrOheRPy3XKsJveqUlVz6I76vrenkz9z6MbxeR763r+rP1Nf9PXddX9PffKyL/pYh8Q13X/8cr\nKe0rRF+mRI5E5Liu65o+e5/+9oVjqet6Ide/cu/d/O0Z/a0WkTUTp/JFwTM63pHrF+f36bP35bo/\nRV7o0xeOlVcP/xhZiUhTPngOfgGehz8l15aN/0xELowx/5Uxpi0iuyISichPG2MmxpiJiPxPN58r\nr5i6rt8Wke8Ske+T6375yyTXvtjvwQdI69zXlVyvuUe/zLnKq+dl1sgHIvLOB/z9u0TkR+u6/rnN\nivTxoC9TIqcicu/ml+sXeHjz/xO53tAsIiLGmIZcS3rHN9+7T38z/G/liwZ+SR7KteXiEX32UK77\nU+SFPpXrya98vHzQHPwC3MdS1/UP1nX96+Va9vuEiPxbct33sYh8eV3X3Zv/OjeShXIH1HX9I3Vd\n/xa57s9aRP7kr+Ayt3PyZq/jfbkeI8qrp/6Qz5Zy/YNFRG4dfvjHyjMReeMDrv97ReSbjTF/eJNC\nflzoy5TI/yYihYj8IWOMa4z5FhH5jTd/+0si8geMMV9tjPHlWhb432/kof9RRL7SGPPNN7+ivkNE\nDu6++MpH5UZG+FER+ePGmJYx5pFc6/hfiHPzoyLyh40x94wxXRH57o+pqAr4oDn4/8EY8zXGmK+9\n2UezFJFERKobK8YPi8ifMcbs3Zx7zxjzUfZ6KBtiruO/ff1NHyZy/WJb/Qou9euNMd9ys+Z+l4ik\nIvL3t1hU5ZfnXK73mv5yfE6urYr/xM38+165lna/wH8tIv++MeatG0eRrzLGDOjvJyLy2+V6Df5X\nt134V82X/MtUXdeZiHyLiPx+ERmJyD8jIn/15m8/KSL/joj8D3JttXhDbvZY1HU9lOs36f9IrmWH\nT4vI/yXXk1v54uVfl+uH7LtyvSH9R0Tkz9/87YdF5G+JyM+KyD8QkZ+Q6xft8u6LqYh88Bz8ZWjL\ndT+O5VoevBKR//jmb98tIm+LyN83xsxE5CdF5JOvpuTKC/hyvQdxKNey3p5c7397Wf66XK/RY7ne\n6/gtN/unlFfP94vI995I5P/0i3+s63oqIv+aXL80Hcv1OstbX/60XP9g/VsiMhOR/0ZEwheu8VSu\nX6j+bfOrzDPerG8VUn6l3Jicn4vI76vr+m9/3OVRNscY8w0i8kN1XT/60JMVRXmlGGO+T0TerOv6\nX/i4y6IoL/Ilb5naBGPM7zLGdG9M139URIyoyflXLcaY0BjzjcYYxxhzT0T+XRH5ax93uRRFUZQv\nbvRlajN+s1x7JwxF5JtE5Js/oouo8sWJEZF/T64lhH8gIp8RjUOkKIqifAgq8ymKoiiKomyAWqYU\nRVEURVE2QF+mFEVRFEVRNuBOE7h+95/4tltNcTLLbj9vXCa3x/0DpFnyep3bYxPjHGuG+JphjZhg\no3Jye9za6eOaZv/22LcWt8fP5nPcq77NbSyz0r49rlZIHeQ21j3kS8Qnk53V6PY4dnCtVqN7e5xU\nOOdpOkUdujgnK9Au2RUFl72gdrFbOCeiz80Sn6co92KGe/3Qj/8vHJz0V8wP/MA33vZls0Dcw1Fd\n3B53Eu/2eJKh3WWC9kWJRUqLvnvQuz12F+jjKpjdHj9fYEx0F4hI0e9S3M0Md5i6OE6T9b6MTPv2\nOO7v3B4Hq8vb4/EY48tfoRlND9cqgwbKUeG3SqOLe/ddjOu0iXuVQ9xrtkRbLELU03YQmue7v+fH\nttKXIiJ/7Id/5rY/R++e337ePsISEQdUzyXG/v4RQsWME9T5+OTJ7XHUwXW8Bca4FBinMfWbHaNq\n9hIBspuHj2+P6wRxO8ez9XzFpY2ti0EOz/nWAGM1ydGucwfbHQYu5m/jCvMrb6FMjXsYL9kSCtgD\nWQAAIABJREFUfTKcIjvGvsF18iOE2+k10P+HDur8bV/X3Ep//rW/+HdvK2NWmPuTEmOqG2Gcxifw\nXp/jFLlOFnBTzoco//E55nJqYd5ZdP2ioP6z0f5FifHhCD73OgjR58zWw0/NBdeqDNbQaonPnRba\n3XExHpsWrpXFqPO0QxFs5tSvK9S57KCsVo36t9voe8vHGvfoEPf9R//xr9na3PyOHz+97c+iRnzS\n1SXqFhuM994Y82vhYa0NMqydKT4Wr4+1Vlbo2x2H5scIY3nngAaJTfOgvrg9ruc4P6vW19qGj7XD\nM7jH05LHDMrR8tCuhobG+THW44RibnebOCmgZ3Qaov+rFt4JPAffbUWYm3mNz//sN9370P5Uy5Si\nKIqiKMoG3Kllas/gzXBh4Y00CGEhKmK8Jec+WzPwy9HpwjLjV/hFWsd4Yx6PcH3vIawC5RS/JDr0\nq8WmX2GFSw55+3hT7Vsov4iIa+Nai0v8rbPEL65mhHIXBm/JfQe/kA39ig6v8H6bh2iL9iOUj39h\nio0uZGtJV3D97q/bk20TCSx/Jd3XMmivFf2yKT388mwFKFtQ4tfMeIS+jFb4JTjYh+WuuMRPqrRB\nv3K7+DVm99t0Ps7hXzV7+/RrTETqc/Rlu49yxDa+9EYH4aayGcZsFZCVlX6FXcWwCiThV+C7JX4h\nlinKV7Vw324H17ET1Dlbon23yeTdt/EPG+NreIy2j0PUc3c1xHfJorhyMNcaOc0jqmfto54VWamq\nc1jEdmrMlcxCW+Q5rLUphpdYMedLFZmZ8e1xQTGYQ4fKEeAeDlnI5guUu3RgFSss/EJO3kffLmJY\nFJM59VUT5/sVPh/7OH/ZZeffXyvbYDyExW7qkoPROco8b2GO5CuM5SzFvGg8wJrz7hxWgIVBg86m\n6JuQLJfNAPPpdAYrUOGhzdsFremXlJ7PxTkiIpaF9eVqhXbsCeZRkcDSYDdgFZmS9bHZxFoTZWTt\nnmNMXArawjaoz0GBeZc8o/rgUSQrsrKIfI1sC+/sye1xVqIP2yXu52Toq5qsZQ8beIZOT9CO5RLz\nZeBiDvoRzs8uMZdreqYtTzDfHaFnYEnPvZTm9QvPzahBz2kP68vjCPVpxHiujXO097jC550ermto\nMaiGNM7p+WsfoLN6Fc5fJejzeYXzywvWTTgV6C+NWqYURVEURVE2QF+mFEVRFEVRNuBOZb6FgUmv\nWcOEWtFmyHwBU7+dYHNmw4U5cT6DadxqwNTboBRqKV2/oE3kwwzH3QKm0SqECdR1cc7ybZjD6/56\nPM7OAKZi2gcu1gNIDNMxyu3QPjyHzMlmhT/UPchKYQrT9Zg2r2clzOx7JHnOcrRjQhu192pIctsi\nCnHNwoEZ1ieT/NCDZGCv8N6ejKjuAcznO12Yid0G6rgkuaga4JqrGNJAVkM6ak3QJss2bVIt0bZX\n2fqG5e6ANsVPYIY+G2HD56PGp2+P+yuYg886KJ89w713CmwubyW493mEugUkbUoD341c2oCeYjwZ\ngUS0TeweSS60idz2MBcsQxtYK8g+UYLvBjHqk13SJt8c18w8HC9PULfWLvr/qsI6kBtc5/ACkvVe\nhDYdpbQlQETyDJuqo8eYpx3azDxJIAd0KlxrQc4uKY2ZKMP5RQEJYJd0xAlttg5TzNl0gTnuxpjX\ndQ5Jblsy33yBvrk4IenJpW0N5JjhkBPEyEb/PVxQ+zqY7/4MclmaY33LLfRrSpLNLq2zZYFHTkWy\nYFxhnNnWupRdTjHPXQtbBLwm7j2kzezRFe69rNBnhhw/PHJqOIsx9x0Lx20q3/GKHKAs1KFNMtfp\nCO2yTZwG5kJ3hTXC3kFZGxQr2lniOCEnHd/HPOLN6H6CsdCx0FdTclzqJhhTNvWBf4Brhr+A8vhH\nXIP1NSsIsV5453gWjMm0U6T4/CrE/PLb2C6SLSBVRiTJlnTz3RY9Uybo82offWifYw36/Irmb/hy\nr0dqmVIURVEURdkAfZlSFEVRFEXZgDuV+XYPYR6cBjD1zU9hoi93HtweRzVMjm4IE619Rh5QIcx7\njRBeIHVEbhakr3VbKENB5vAeeRu17Nfx3TdhxoxH66EmXMOmctzPJVlxOac4GiRD+uQl5nbRFgl5\nu5Rd8v47RVmn+7hOu09eTwVMlJcL1NOVdUlrGyxyDJ3JBP1kKO7MiqTZwwzlHHch1U0WMCXbJczT\n6YhM+wbXuWehrdw27utnJCO1IJHlKclXOXkCGgq0IiIVxSi7JFPvIsN5swbF/fLgMVQ9x/mTHfTN\nGZXVn2BM7FPMpYmL87MFTO+xR/FxSJKpCkjl26SYk+dKRfFeSJpuVTC3FwHauP8E5T4lR6wsh3nf\nusD5wQGu75O8OllBmssMSSmkVK28d26PowW85QbkzSkiUlgUp2xm0+cw6a9ifMcZUcwjH+NweEXS\nQHR4e9xKyFOV5uluTR5GPsdjQtnaFNOsJllwWyQTyIt5A+1YLjBmI4q5s8pQ34jkoiRBvZptnH8l\naM8yRB0jB31ckQw+bZBEGGJdqijWXkbrRrQWfU7EBCi3S3N7nmFweuTgPKEYVz61e0meh+fkwWZZ\nr90eLykOYTelGGMZ+tsEWE+nCa5jZH1N2RZlhjHSNiRbsuf0FcX7ovWidFDWpqGtAx0cGxvt9fnP\nk6RYoW59F20xGGAdGK8wvpZdSHMhxQrLrbXgZbLKUabihMZAi9xuPYw3f4FObJOntn2A/kwq3KPh\nkiegQbkt2k5TlZAInwvFNyyxZhl73Qvxw1DLlKIoiqIoygboy5SiKIqiKMoG3KnMJxSq35CkMyKT\n+QMLJk2Lwrxf0c79RkDeGgnktf4uvFJ+Qxem4WQOGWKWUIBJD+Znq0nBL0meEkrL4rrrMl+awQwa\neKjPZAETt6lhxvTJLJ14OMdKYfpmL5sd8nw5S/H51TlMqCUF3GtQAM+Q7N5OuJ6eYRscFjDVuqQi\nTlZokwMKWhhTIL4FeWfVFFzRczmtDqVWobiDzS4F6iTpLKL29CltjL/CcUz9Vxi0m4jIhCTfToi+\nfBChb0KS9sIIY+eYvJ7mFNyOvUtzH32zIPnEIhO7rHCddAGZaidEnaey7lG6Lc7n8ETq7GKe9hwE\nq6tovEeU0umCUj/0K/IqmpLXFwcYLMjLs4Njh+ZyRt6PCxdjIc/wue+SRyFH5hSRnV14/ZRNSnkx\nJhknR32Gl+gfR0hyqh/fHlu8FkSc7gZjLLJxzVGKfo5IJrNaWI869nq5t4EhCdsqSJLrkTRJskjl\nod29irZikCdcbaFeywal56Hgus9ckvBmWK/bXfRZ30KwVytA2ewVvpsLbdEQEeOhvWryxi5H+H5z\nh+Z/iHm6mGMNWpFEFFI6pPQepQ+idF6S4vwGXdOQTDsr30d9yHl1m9hz8gqlIJTWuxj/7QDrmUXe\n8UL16eygDu2CUkCR93KDtpwEJfqqt4fz2xR8m73uGn2MBZdSiflm/bk5djDnU/LIjGh+LWkbUEyp\nv5ynT26P8wHGSUgBeBcLkmf30HZFTenjaBvAfh8dV5FsG05f7rmplilFURRFUZQN0JcpRVEURVGU\nDbjboJ01TOzLBGZdK6QgeUJ51XIyXZJXgkMpr0dkDvSXJA3RTvwmeXoll5SriXIEHUQkPZA3zJTy\n9BXOusQynNK1SE7KA/LWIfNj30V9Tikn2TiDOfVBAM+S2oGZsdmBl6NPOd8cynbvejCnsmeEbW9f\n5psamIxHNnlxPPyy2+MxBYzzyCts16DvZ4LrdI9g8vVWlCvvDJKBR95vVpOCAVKAteMc7emSp1IL\nKrD4k3WPIZvacR5iXAwK8hhyYFYuKYCrn6OtHZKzCur7FslQLNXNKdZkQB5W++RRF1N+yHCKttsm\n93ZpftUIKplNIcM2LilY3wCSrJOgvS8m6Ks9yuHW3EcA3jjmPH2QS7skx707wX1DyrnZ6OEca06e\neXOS5kXEtckDKIQslY5Iks1Rn6NdfG4yBAYNaJywV+msgDQwoDHJ8sR9CmC7pHyE5QKdfjLDWrMt\nrBbWhz0bMu0yp20WHo5XlDO0JolXUvIcpW0M7Sb6bEj5Tb0VPMFmIXm20do9nNO61KOtCDakPDdY\nX2cbFY2XENdKHJJ2WuinPu07yCkPZlSg3f17FDhyiPutSlpTanzX87C+zGdPcH1DOei87XtmiogM\n2iSxDlFPu4HxmxnULV6ijXsUXDZfoL0qh7bTZOiHHQf38shDnT3lf/EZ5nsxxXG0g7m508ZzrEm5\nNUVEggXK9NkO5hFNZ4mpT1La5hBQUNHVCcZb3KNcq5Qr1accvRyQ1X6OYMymRt/mlNfwMn4qL4Na\nphRFURRFUTZAX6YURVEURVE24E5lPquEWbLfgnnPk4e3x+6ITJptvOvtVjAzxlTsXcrbVvgwq9cU\nTCyO8fmDN+GV0COvl4rkmSWZISVFeWp3XS7rJCRb+jCV7pJ8GDfJvD9C/YOCPNGWMI9KA2bMpk1e\nCQb5xT5J7XJBr8NZgPOPLnH9xdH2g8k5FBgtvIA5OM/hYbLKyEvCoB3CEDLEzj7Ms/4hzknfhWRb\nR5AbVj5kpGYf/RFPYZJ3cpjwG3voo0f3yTvSR/uIiFw9Qz+3lui/uoV8fJYHE/CYzNuS4vywQxJZ\nRlIdmc9n5I1JsTnF76PcwwreaN0V2isnqXKbWPeoDylArNVCOzXJQzYi+e/q7O3b45C8svqPMNfc\nCm3fJXnNLiE3jA3M9q8b8mz0kGurSXqxRYEh7ea6x9A0Jc9LkgmTMcakY2MsHZTotyYFHl10sC5U\nc1wnO0W7rFh6JM8++xCTc6+N8Skkkc9Ptu8Cdj5FBRwbMmLUQ7uvSLLsdVC2hLYozEkir1cURDGG\nrJ+TV2NK63JIgYKnFDjR9ZCLcHmFa97bx1hJhuttkmYYjy55hIcNGgsOBfa0sd6FFEQ1OsI5U8qj\nl2a4Zpc8ommYyjiDLFQkOMdaYn05u7/97RQiIpdjlLXF9aFg1Cl5uQoFpvUEkuyK5PUp5e9zezQ3\nBXJ8bWHsnNKWltkV+qMiibA9JI942mqRvhD7cjrG+Okf0HYBC/eofczHiPZCrBx8d6dBHpk+eWTS\n58MRytoakJzdoDEWYxtFu0C75D7W3Y+CWqYURVEURVE2QF+mFEVRFEVRNuBOZb5xAjO+LTC5NfoU\nrK+knEkFyVzk0SA2mZ/HMOPdW5LHkIv3RJ8kqQ5JQMcUbLJ/BVP33IIJP6FgmUW5nv/LSij/F3kc\nWC55J5JnVGsAU+xVTgEdKTjeJENZM8rt1qSIcHYTZskuecrMYtThOQdrewVOJv4R6r6o4fWQj1GX\nkIKihpTX7WwG0/BXUd/XU/R3OoaJudFDH5gJeUIlMGe3OhhPA4EkuqTvPnsPbZJY6958nYja14eW\nZgLIEmcFcvMtYjK3+zQOPHJJMfjcIynh8DWSW94mj5SSgtM5CAY49SCDN4UimG4Rh4Lo9nZQvvIC\n7eqdYV4UJJelJA05JH1nlNeu66L/6/zd2+PAQx7MLiUCzCm/WrCH746eIX9fRHkKKwr8KyJSLylf\nIM357i7O25mSPEfSVbug3I4kz06WOJ4blDUiKWFEMmRWo72CPXjjNmhLwZ637um0DSoKZmso92Ee\n416tBtarFQUZLiiXZZ889SY5rvk0ofxwHsm6bfIWq0heYkk957WLcm5S7rexve7hOCZvTNvH/eol\neWeRRF7R+rIXYhzNqNw7TczNMsK9k5K8V0ucc3WJdbZLueacFtbrKodsvE2KGW01aUDOWk3x+YJk\nrg55Ngvl10sN5sTEw3gfFPfoc9qCUKOv8hjbLrwC5zhCXvAGY6ogqW1BXpQiIpZPeSF/kbYU7KGv\nOlS3inKwtrrIjzkIcO8ixBhrC601BmtESc/vZoHnjqH+NBTIt1O+XB5UtUwpiqIoiqJsgL5MKYqi\nKIqibMDdevO5MEVOhpA3OB9U5ZLnWROm26yGrJAnMPvtduEFMiOzbzWiXf8RzMlPrz53exzlMOMV\nDx7hczKTn1F53BomRhER/wCm3yIh755L8u5qUtC4HsymHpm4yzZM4uUYdYsryEppE20Uzci0TPJU\nk7wNL0LKG+hv3wVsegHJoEf5uWLywqooL9hgQN42E5T/7XcgEXZIvl2QFGLllI8vhhn+EyQvVeQJ\nUsbwvLGH7PFyenvc8sllS0R2dzCOmkv0wRmZxhsk7RiLPGMCkoUm6Ms3ejgnJM+uS8odJY8RIDKK\nyZOV2s5c4PMsXJeztsVwDumtu0QdYsrHN/HJA4gCPe48onmdYc5ejCjgJXkYRfcw9rsOmeRrkhLm\n9DuP8j2ac8p7SeO9M1h3GWpQ4L7GBFJt2cb9DkjSuqC8iA2SDGeUdzEl+akZYj6mR2ivhMbYz1H+\nt08eY70b7GDd8Po0FrZEkUPOahQoQ0CBQ2tq3hblJR1fwgsr61COuwxSW7AL78qGQMKZ07hu7kO+\n8WvKu0e53B5TPs2yA2m+Va5L8FJiLkSCuTakHK28A2OX5nYSkEcWBeBNKWipLfhyQQEvMw+fP76H\n8XUxx5iwKZebJVjft4kXQz6sKZfnIQV4vrLJc3pBW1xo/LYpaGvl4rtVC3UztL5MYsqvZ6EPHw/Q\nVxZ5aU8pN2OZYkwFL8hl4xhjbEzBjMMR6uZ2cdxsYhxaVKbGgIJU01aWWrCO0C4SKQuMt3CMdln1\nMSbdS3whSTRop6IoiqIoyp2hL1OKoiiKoigbcLcyH8UVqyifWVHAFJsJBbbMyNOrQSbHCGa5Rgvn\nhw7MwalNQT7nMM8XU/K2mZNk0IC52m7DdGkvYKIMi/Xgl8sJeYB1cV6+xOeLCGbpJeXhKijwWUq5\n4Lw2BTCtYDYe5RR4cAozpnMP52SUC6zVgmxV5uS2uCXMCubgFeVgG6fos3aT5LkWTLWOj++aU/TT\n584gx7x2AAmvDGH277qQWqMHZOYnr6uQPAp3SJqq3kR5egk8P0VEYvLatA5R1t4VpkhBweZMB+No\nPqOx6VPf9CCH2A76qb+ETToiZWBCjl1Jij7LSA7xBf26TcIC9Vx4MMOfX5CURuekDtr4DQceq11K\nkZWRPBfsoI2+MnyMkzjAaoCx8/wUgQrnc7R7WmOsiQ3JvpejrUVE3nwLwVar19DIlJJMRhWkR0PH\nzTY8DMWlILQzlONcyJMsx1gNfawjRwP0lUXBBk8ob6YnL+cx9FHoUJrCnCQSU1PeOZI7o4z6bx+y\n9hUFS2QPVI9krhkFCvab5DlI67hvc3BN9HHRwXjaSbC+nb/gNV2Rp9qc1nJDASwLClKckrwe0Hzx\nSWo2HtboMwpmWdRYQ6sZ1pdWCwP74Q7KekXSUWOGMbtNPMpNOGugTHb53u3x4hJSc4f6YUnriHFo\nO0KH8tfNUIdTmu9NG5PFjTgPHsm5FJi5T1JgMUd/1N11m82EZPSqvqJjyIcj8nBv0vYa9qJtpZQL\nckbBXBPUx7qPce7P8d2Fiy00/hTbYNII4z8fvlzeTLVMKYqiKIqibIC+TCmKoiiKomzAncp8EbmQ\nOJSHbURB4/olzIwFeXeVVzAP25Q/aRbDpLlLXglRies/G1GetwI28MSD2XN1DtPjYwr0FZAsMF6s\n53NLyIvraIXvLMmrIc9hKl9QjsAqrulz1KftwRNtJeQN5cFcGTRgcp7VlEuIzPjtHuQzO1r3XNsG\n7xcw78YnFJTNQztkC5Jpc7SvT15UZD2XIXntTDKYW7sVmacpB2LTQRmSNi6UR2i3fgLT7pMpvrvf\nhKQoIuI5aLuLGfrZbZFXXQlTuudTHaisGZ3zzgmCVlLcOclIgu4lOL/hkbRJecFWU9Sz0yWZa4uE\nFNBxSm3pBZT/LsHcPNpDWVt9Go8ktbfJa1NOYDLPybNzQGNzt8J1Jm2Up4rRn70Kx8eUB7Cq1ufm\n6n3kC5xZKGsvIs/AFdrVJe+/cgmJgVQcCQ8gJVZzdGhdo11qH/05pXHUukdeS88wXsoe5ezbEpch\nZM3mc4z5itrUULDQJzXK0LkiiS3CetrI0U/FLvrVG9O8o3xnnSk1HHtot0iCIy/d5zX6taQ1WkQk\nWaF8tMRJQ2hx3qN8j1gSZUjBi30L5R5QQOH7FsbK0EDyKXwK0pqTVzZt73Bpi8N08moC6jZbJFtR\njrxyiT486mNOzZa0pcDGs29aYZyWp2j7iYc6lykFoCWP6pq2mYwCjItuTvk6KTjrBUmnXW99e0yR\n4W8lBfccdSjw6hR9tbxPnqQxOveK5M/+gDyeR/g8voAUWMzRV80OBePO8VxeJhh7efpytia1TCmK\noiiKomyAvkwpiqIoiqJswJ3KfDMX0ptV49Z+SrmHbEhGuUXSFpl0veDg9rjXwOcj8qoKDD6PQ/Jo\nKeCpU44gqc1KmBinBseOh2tmHeQwEhFpu/DQWZDJ0TcwiQYnqNs7KQWEW+G61gHqPC5JqswpBxIF\nMJ14MFEmFdeZ5LwMZbCX2w8MuN9GrrGTc5iGg4ByPrXIXD9Bf1g99Ic9o+CPrPlRLqiU5LzzS8hF\nSYDx1CtxvkteKO8e0n3H1M476yb5FXmq2eRVNk1RvoxyZHnkYee2IW1OM5iMV+TNNluhHGYf7TK6\nhDfLLsl8fgpT/bIJ77oiW5dAtsWihrS1NyRPQpfy8Ql5hdrwjHJqyASBhfF7ZaG96ymCWXokATQf\nvXl7PDdor6MG+sClvoqfY04EKXmARS8E7XRQpmSJMTPMyEtoAI+/eg7Z990xvLIyzhcYIsCqkHdi\nr0/eeTlJ853P3x77OTzmcvL4q2eYO9si8jGWL2eUx7TEeuct4TXsGArUSetmsaQ+8NHf8xoecpWQ\nZ/USbbIkSS2kfJJBjvIUFETzoY81/Zg8uUREPAqwGZJXbFQhf+Vqjv7PK8hKYR/z0XMxXlxBncsC\nXmQuPU9Cyh9bk7wkAa6fvItckfkOuatvkekJeSHymvcAHslOFzJnt8b6OryAVBeOOWgrtiC0IsrN\naKHPQ49y6NW4zi5J+XaBuTynvLKmS3kdU6zfIiI5eRsGHu63usI9lpTXcjyHJDskD/wjkv/NgIKE\nUi7H6n3MNStEmfIzcqM2uFfDp/yw8frWgQ9DLVOKoiiKoigboC9TiqIoiqIoG3CnMh/nRgqaMO8t\nDMzwdQoTZUUyn9+DGbPhk4RH+f6sGczDiwhVc23IJ5EFU+9sB2Z7E1AQ0YqCbtL1ixqmURGREXll\nrRL87aiEqTDbo8CbS5iK20cw0a5GkBjYK2OXvHKGT1C3aQPmx50AJtD8CqbraU4eFPsvZ678KIQR\nJM+I8gM2bLTvokR9T8bo+8YCnzfuUYDUIczZ1Q6Zgi0cdz4F03O6hMQwj9Dmey1c36rQ912SF5+9\n0CQdkkvrEPeYsdeli3G3l6DccQd1Lk/QZ9YOxniX5FtynJMpBW2MyXWsQ8HjZmcYy83B9j0zRUR2\nKG+bu0cBUF2Uzzklz0aH8lztw3zeC9Heixbm9ahJHkMl6lNTfsWoQ8HzSFVpuujblo9xd0nyci/C\nPBARKY/Q3hUF3g26kAZyzttI0vzIJk8fzguXop4797EezSyS3QeQFdrkUWxstNfKkBTIg2FLLHPy\n2otQ/paP/KM+y1mUv48lwkUOufNiBWnv4S7qckbBGdnbs02es2ZAOVOXkLUXBr/lIwt97NQvTE4b\na/OKvEvdAmvoPEWdJ0vIfwcuZD7bg9SanmO9fmIQ/NWeYqyYHRzPyKO4TZJ9SvkLew0cb5PFM/RD\n7zECWPrn5C1aoKyrPbSXTVs/0hDrZXuG85s5baOoUIcVjaPIx3xqk3fmvEtbSGitjEja7bjr3nyz\nBeS8nNrywQDzf5VgW4BcYVw9pNyv0wLfbVdYs/Z24HXbp7bI6L7uPupczDBP353gGWquNGinoiiK\noijKnaEvU4qiKIqiKBtwpzKf04D5LfdhfrXIQm8qCoy4hCk2HsGkmbl4B6wWkIauyFOnJ/AMKxs4\nf1mQlwCpdrsk/8UGpsuiBTPppFj3irsib8BJAdO93cS9nRim1d4SJsoGeYPZPaobJWtLKphZvT3K\nybSiXFdLCgDYIm+rHGZPdz3V1VZYOpTnaIyyXZLZv3UIyWM4JzmLPJhmZD72GpASmiuYW5uvk9Ra\noo/zJsZQ3ULfjEL0mdVDm3y+QFvt1euy0LOfR3A3f0D5/ATnVSSBXJHX3vQZeW+2qG9GlIPQJSk0\ngLxcBBibLQefpy7ljexQYEcb990mwQEGSYM8hmiqyULIQ+eEvHuWqP+cctYtQ8zN1i5M+B2DfjYk\n/y3YwUbIDB9RgF/K27b3AP18Wqx7Z3KAYIuC+bo+zP7nLqSEgnLYNX2M1ZMxzvEb8Bg1CcZCdYiy\ndh18HpP04vQpD92U8tORXLgtGuRt5ZPs2u6hgdeCkbqUf7EiGbmLPg582qJB8soDmmsrkuMdyv23\nrCBN5Snm9YMdeFZ3I/K4frbel40+nhuTBOvsLMNYiCOMtWBGW0Joq4gUmNezAud3cvTN1Ccpm/K4\nziZPb48dhwI7Uv0XM8yDbTKlbSftFc1BmiNZQnkBY7RlOqe8g4YCXlo4fkRSdlPQ1i5J33aNe53M\nsK63aPuG49Iz10UbVZ31B1CSUZDnFOeldA/fwpjp7aEO02Ncq11hXQgjnGMvKKA0rZc2eWSmFvot\nSOi5sMBz5DhTbz5FURRFUZQ7Q1+mFEVRFEVRNuBOZb6ugfnNoZw+kxJm46aByc1vUvCtGua6fEHB\n4cjz5pLyJ5W0Ez8mKSGh3FOtDq5/GSAIpXUPXi/NDskFZ/BEERHJSK5J3yaPCBvHA/JoKymfnV/D\n5D5zyfxO+fjGGeX8ovxJj2uYyicumdkpb1nvHspmZ9v3APNTyG0Ur1QSkletlLzw7uHzIeXLS07h\nSbNHUl3aI4/Kcwq2tg+zvWnDzLtmYB/AzG/II6npY8yN45y/IasBypSRnNdNSLqgQJIT5jgJAAAg\nAElEQVSTS9Q/nsBsHdgIPliSfm0y3PuKpIT0iqQHF+Pr/hHaK6OxnO68GimhTcHq7ALyzowclJIJ\nBYBsoI3KfbTl288hn2QUqLIFxUtWGaSnzwx/4fa4t8TnhxE8tZIA9115ML2fTyhIor0e6PFzFzTv\nSP1+SAEgOThv3cbxcIEx1v8UgltOI9wjqyDfXx3jOOhjjDiPMKZ6Dtad4CHm8vkFtgpsCyujnHo9\ntOlkTnIObYmYkoQ3J6/k1hJlviD52rHRNyXlrysXmGvd6FMo0B7m8uUl1tnTCuMmoQCJ08X6GI9p\nHbRIVhpTwEjHRp37O/i8TvDcaCToV6sBGSkq8HmjjTk4nuNeBzbKPZ2irG1DwWLJM3WbGGobh4JL\nDygwZj0n70nyJp85FBS2hLzeWqCeMW0tsSkg6ZJy873WogDaI8pjusIzpxXg2HdoK06BNVFE5FMR\n7n0suG5N21oqkvYnFZ7rk4y8wgVzx19CnixJtrMcyilZo6+aCeZsuML6nVBgz/Ald1SoZUpRFEVR\nFGUD9GVKURRFURRlA+5U5rucQAIIezA5hjOY/SwPeZKiNuxsCUk9R12YU49TmAObc2gSbg5ztUV5\nx2IP5y/9126P9x3IAsNjmDejGpJiGK7n5tun8jWjJ7fHi3+I+006MMuWDZTbIkcBE0CW8i2YxBMy\naVYU3LIoUQ5DXhnZJc7xOUjmC3notkFmQYZa2ZAdgz55upA3RJmij+sFpIFVRoFJKfedleM93xOY\nqtsu+uZRD6b6ywHOaSb4bpyTZyYFfEum6/minC7GnSSQSJOQ8sKd4vtzC1LHg8eYRsspSbYUnHTR\nR1mvTmCGrinnZG+C60wCmJ5PyYPJX72a3z8PDiE9zSlA4RXJ4kWDPHQKjLUnFPRvRjkYG31IBns2\nvntM8919RnnqIsyndEpyU4V1I6Y8gGUKWenZdF2CP2uj3w8oKOUohwy3ewj5wSFPz8xDfZYt8iQl\nz9OUNIDkkjz4aMvC/Rptlw5xbFcUvPdw+/3pPoKXnJljvljBs9vjskb/OSR/H1SQXUIHbeLQ1oXL\nDP3hz9AmdZPmTZe8+XKUISOv6XpM5+AUyUjWERFJ3oPM7XcoMCjl8HOnFPz3kNZsC+0+pOCqb9EW\nkiEvBRbW4ib9IU8xTkkplroP/fqwD1l7m7hD8pD+cpJqXTxDnT7KfUHBscMhzu9SntIhbTtwcqzf\njRBzwjKom+9hvR+8hbW2+CzuG1LfuCzz7a33ZzTHWtsokL+yOuNg3HhWLg9Qz6CPxo9i2spCz/iA\nnvFxhfXbS9kjFXPWoW0dFnnc56fkdfgRUMuUoiiKoijKBujLlKIoiqIoygbcqcy3oIBjIXmHjKYw\ny/abMN1bNQVEsyhPzvzx7eEDUmc6FNjSO4YU9pTMwd0A5nnfgTkwaUOeyYYwgZ7EtKXfWg/aKT3y\nOlnAFGkfUf4/ganwHgUq9QxMmjYFlgvI+yDYgyRxxt5jA5KGHJhliya8G+oeylqF2/fmq8jLLY0g\nW4QUbLJI0I6XlF8p4nxWAtnmxDy/PfbJ3HrYgydN7CDv1pRczSrKgejbqHudozyX5P3VStd/R8Qd\nmOjvUy6x5Qn68oLygh2SV+FkgnIct0lifI7z25RH0CZHQtvCPCg9mMMbBmPzUyTB9dktbouY6Tu3\nx0UJ032LPKZS8s6MKiwds1PklswoKOaDBrwZp+RhtKBAuzlJe30L91ocYr4/+RzaZUV5KWMX424q\nkLBEROoS13rHRjm+1kZfRRECcl4sUaZpF/JOmEImWgYI2rkbk0zQIe9PyhHoNjDeMvJGjiLIKofk\nSbUtihhjtkFBim2aU8bBOY9IFgloqSjIi6w5RrstKLjiiLwRDx6+dXs8eQfzNG5gTJS0jcOhvHEW\n5ePr3IcHpYiIKSlHXExzU9D/vk95GmdYN6chxkFXMKaenZJUtcQafRZTWWOsuacV5MzIgwTlUPue\nZC8nC31U8ohyZVoU2DjFWjBN0J+hTfnrDtBeRYmxFoYIUjwnyd55Bnm9jNA/RzHa0Rzh+D7leKyE\nJHEKlumcU+BUERmRJDe+QJvtL1G38RHKHdood99GexsP9bRtXCclT+1AKP8m5V/NUqzHro35u9fG\nO0Tiv5ytSS1TiqIoiqIoG6AvU4qiKIqiKBugL1OKoiiKoigbcKd7pnYDaLwVhRzIaP+Ut4O9TpMz\n7FFoedCs2w1KRhlhL8Ygh7a62oGuXxU4P4ihA/sl9jf0WijPhDTUrIaGWpv1d0+3gEa814PbbbIH\nt/niCbTplKKhN/oUWqCClm+aqINFyYpb1FPPL1GmIsD+i24Crbgm/13a6rI1UkpY2qQ9Q4uE9g10\n0aZdG/sY8pyiU9c47giFSaiwfyqh/ht0cE55hGt2HOzFsAwlhW6iDLsUSXnZpMjFInJAfZNXFN6A\nxtp+B/ssRm+j/lPBmGocQ/dfLlCOUxf7fnYLuPTufhLjwH6Oa9qGkjjXtL+jg/pskyXtIbNtTtCM\ndto/wL1Xgra3V+QeTdHK8yX2SmQ59nQsd3i/Eebp8Ar1rGf4fDrAmCoDfHfooz+raH0/43SO/WqL\nJfZo/J0u9vI8pISwHUqaPbcoanRK7t4xxsWCElfXAdavdkRJabvYh2dT1gKr4iTs2+/PskKYiFPK\n2tCm/SZdQ3tmYuyZsVysY7WF+ta7GB+NUxoftKZ3G5gHc4pUz3tlbVrr4gmF4KjQxwfz9bkZUNiE\nitrdSdjlHvcuKbN7EVO4FYp472cYm13auyURrnk5Q7/ep+/mIaJt7x5ivscUYmKb7Aa4h0sR0F2X\n9iE+OaXPMR+DA9pHXNMcyRCqxeqir/o5+qfO0S5ZgXEUYYuk1Ab3cinBtptSSAKKbC4i0g3RTvmY\nns0H+M5b9PzyaU/ucYpjj56PFYVICmuMt8JF3fYO8LwffxbXCWjNSmm/7U6giY4VRVEURVHuDH2Z\nUhRFURRF2YA7lfnensN8vpdTpFEXksFqDnnn4AgRynNync0oYfLoKUkgFLHXomjjD+9DdqtOYd6N\nL2DSbhYwOacWTNdxB2WTJ+uRXB0hKdHAdG9ilMmlaMd+g8ysAd5jWzOUqXlCISMGKFNOOTRDcg8O\nONExuTtPU9TNxC+ZsfGjQLLoihJiVj2YXtsRzLnBlGSeFUebRvmjFr67W8DNOj5/7/a4G6KtWmPU\ncVJQ5G1yVfdPSLKlKPcrgcwhIjKwKeHmAmOzH2EszEguGpN7//Ez3Hu/SyE2yHV7r8J1zC7qmVPC\nTZuixydXkIVWDtq3R8lKt4kVoqzisMs6SWzkNt6gMBEJSb4RRRsvCrTxylDS532MX2tEkY4p4e7T\nX0SfzyeYT/ZDhMkwfYzrq/fX+3NUk2v1PiSNZPz27XFzD9Jw58u+4vZ4j+swoUj8DfSVoSS43Q65\ngTche3RpzZrT/D22sMY9Psf526IuKcH0HiUoHlF4FRv1GoRYx1pdklc89PeMZDTvEPXdsyBZWw3M\ncfcM12dXfYeiyC8WmL97lET84f66XHZBGRPMCv3aCij6ekSJqoeUqDtGyIyBTbIQhfzIMtTftVHP\nTpPWjgBjqKLMF6RmiVO9mkTH9x4+uj0+omTw0zllWDDoq0Eb47RB4WPGLUh4doU+aUxpH4hP45Gk\n8oJeFSrKCnFEoUZmdJ15ByEW8np9zbqi5MgrwXfq52jXkYt156FFmUPmmFNXP0fbIvYpfMYuJRLP\nUO7wXUpo7KD+IwwLOSDJN2yuJ0//MNQypSiKoiiKsgH6MqUoiqIoirIBdyrzpWxODSGHPAxw7JJX\n0SKGGdPM8d2gjeO912Cq/8wvwCx3tCKTcwmTXoe8fpaPYNJMl7D1uXOKRD3EOXaLJD8Rqclby1vA\nnGw6MDk2m7hfksCEaJaom+vCVGqTyX0wwXHZhWnV8mEabVOU2mcJJbhc4jjLEFV+W8zfJ4nTQdt1\nSRYLqd2bA5jkHfKwmVYw+ZYZPm8MyNszwfFiiv4o+9RuZDpeWJCUPI8SzNr47mMLZRMRaU5Jxihg\nDn6aYSyMnlGk9xlMz2+0MS6WK5ieH70Br58eJf4sMtQnoGS4klJS2gKm+oSSSreD7UezFxEJ2pRt\ngBOPTzB+Vy14zEyfYjyOlpA5WxT121T0W+0+JbUdo00rG9cZ0DpQXKCvkgEkVWsXxy2Sgyp3fW4O\nKDHr/UOsEfUJzmtTpoJBF+enBfphMoc0cpbhnD0H0uNyRuO8xJJ6TF6RXZKMBj6kMecVOIBZEep4\nSZH0DzPMTZe8nGyS8xpNrBvzIXmIpVhbTQtZFwJar959++ntcT1H284rtH/lYG41mvDc7vcxn6p0\nPSp8nzzJ6h7qMzuj7QK0raG7RN3YIzpqoN39Nuap6+N+oxP0pVf36ByU+yJBGUpKQlw0MWe3iUXP\nrOcL9MNsjPE1fRdz0Cww75w3cJ3mDH3Spaj0AUl7PkUGz3Zpjk/xDLFTjOXEwXGHkjMML2k+Vuue\ntvkM9zjLUQ6HsoJc1Dh2qUzeGcp6HGK8Vefof89gLD2P8d24Qnv5e/QOQecvF/D2raYv52mrlilF\nURRFUZQN0JcpRVEURVGUDbhTmc9rw9zX7MGEVtow0flkbq8yyCpnc5zfmOEdcNinJJCUZPb4KWS3\nhwFMt6WLKs8p8aEn5DlHMl9EHmC7DjzMREQsCg53IajDGxXMo2UAz4Iyh2nRCmEGvVpASkgsmJMr\ni7wpnsJcOaPgiU4T923kFEyPpMA43n43zwzaq0XB6pZTeHFIhc9bD9A39RUFPJyhvq034DEzIrnM\nGkEijGOY/H0LMoEJ0SZpE2Xr2+QJ4qEM7nw9YXDPRz+FHiSc2RN4lT1FkaRN9165aPfXPUgGdknS\npoe+bPTIo2mMi57M2XOOAmGSPJ7G1L5bJCSJ0VBwvwUlh65HGGutJdq4pITkhgIJThqQ5LwYdfZK\nXGecQiZZNiiYH0lw+4ev3x73XodnkNVDm8Zvfm69PkuMh04bskzaQX18Cph5LOSB2yTZdgdyw2qI\nth+SojPI0OexhfrkZ2jTLMAc9+7jvq9CGLok7899F5Kc71Jg1gbkomWG42eUnDsvMAZjG96S/TNa\n395EUuJHPch5Twuc07XRtpYh+SdCX6wokO9iuh5leEaJ6v0lyuGSR5adUNBkksjndK06Jm82Digs\n7NmIz+MGxuw8xXG/hNfZ5B7Ody2sX9skMOSdnNP2jRptXL9Gga9DPJdWIzzL7Ba82imvt+SUPHzm\nY674E7T15AnmQfk62sKirRwVrXcVJUB+Pl+32UzbkPYKdJuYDtaRNgXeXDlYw8tdrC/tABpmx0Kf\nt/axRjwcYrwl53gnaI1xnfMe2ojX2s7By2nwaplSFEVRFEXZAH2ZUhRFURRF2YA7lflaVzD9xT6Z\noncoL94F51VD8V7bJ48/8hJqWxRkzKV8Zgbm3SXJXM4hTJG9HKa++RzykdPFNeucPBc6615x7gzm\nwT0L98vcX9rTJ2zjnPEQ8k7unNweR5QPKyWpru5ClupTffoXMNFOpjD7XlQUVJK8ArdFz4ZZ9Rl5\ns4VD3LcZweztrsgb0yfJ7wgyRL1CXXoxrlP45P0WPb499kiCSimAnZdBIkooyKfloF8rWQ9kuiTJ\nYZSz1EGBNymY64r63lyi3P59BNiLWpDtmuRJOKZAriWNU79GWcsW6lAJTPjVZN1rbVtMRxhTJqKx\nTPKXQ7+9lg7K6lP/RCQx8G+1rIbEktEcfM2GrH9+he86FJCz8xDSQ/SIyhbg/Pvhej63AeVeyyio\n7OgEfdsvMe8CCj4YUJ44mzwYlyckexWYv/MQfbXnY1yMY9wrq7B2VJS/0be3PzfHObXXBPOo3aH2\nXWHtOxtjfO1UaJNpSv16QMFoA+r7hIImNzCXewbXr5ZoB4fyt7VCjI9VH+1ZfBZBTUVEppTvMAjR\ndvcTzKMJ5U0tbNzjoEHevD1I+fNzjK/MIY888vJdLSAvGRpeZYnPvQnuNYpeLsjjR2VAOQ+rSxov\nfawF++RFml+ibs8LHHtXGAuGvNz2+5Sz0GCNe/oO+iFuYQ69eYFrHu+izr0F5nJGWxayAPKviMhu\nCG/rkx3c+3yIsZQUFISTZFXvEa3557i3TUFYR+RRHMYYh1dj9PNzG/P004L50jrEGG66LyfbqmVK\nURRFURRlA/RlSlEURVEUZQPuVOabjEm6acM0HmQwm2YpzJgUC1AGXZjuugFM8i4FgIzJ488KYA5O\nyOvh8ArmzTKlgIQxyuPEMDk3HLLvzl6QWOg7owXlAovIW6eEuXMKJVHSKWSPXgsm9Ac9mFkvLNRz\ntKIAZZTjb0aOL1cF2i6uYbo0GbmhbQnXIy8vcm0KyHtoZ5+8TVzUK8rgwlFT7reMAmx6PvKLVU2q\nJAV89Ack5ZYURPQK1yyoTewBeY4t1/MsrjxIBrPnCFb4hPJTtR+iPxpLknza6O/zFP20t8LYKcmU\nvmzCbD2coO16NMarGt6C6Qrm7Nfuo1+3SU35xuZDtFmbZLhxjH6zJiTdHGAZSWdoi53XMAcr8kha\nTDBvGh2SfVaXt8e7Xcy1Vpe84sj77+EDjDX74MvW6lPM0ZbHzzEeDu6jD32a/22qm3sPXkL+CGNh\n3IL0kGcod7pEuY8pMG8U0prQZBmRcg2GtMhtidYCZfA7FFi4CbnFGv/s7XFGOSenNdohNajXG/kn\n8LlFwWWf09YN77O3x04T83dKnlkyxTiwE8hIdQNlcynHn4jIPgVttGntv0gQYDGw0JctGlONAB7Y\nbgu2g6c5yYqUv86he+/SuvOspEDDQ7TXysdcMSsEBd0mOT1PzA7moyQU1Jmkx6GLzwsbz9xVjfKN\nZ5Cdl0PU7RNv4Trt1yGLklO7jGusfacntMWB1sEZeVpeDNefP5GN9nN93K/zFrwNOznmXSKow/6K\ncuAusWadkAd2+pwCcFNQ5KzJAYJRt4iCPz+mLQjLnNr6I6CWKUVRFEVRlA3QlylFURRFUZQNuFOZ\nz43o3a2ASXBowXxekHdLNILXiJAXSFTBtDxKyCtuBenMUHC4JnloVAbyTjFGeZrkCTgnb7OlDbNv\ntIBJU0RkuUC53QDl5vsFAUyXVUZygIXr+juQQ9IG5LC2g+vvVa+hfII6xEsyoZOU0CHz6yrfvgfY\nZIr2ehiShEd1f3aF8rf2KWcbmVhdClKaxce3x+clTLJHLszTEQXtlCvy2iFZ95QCsk0CmH/bx+hX\nDpQqInKQoxxnCY7bBuP0DQ8S28jFGCwE9T8iDy6L8hSmOc63KbfXDgVYHESQOt6nfk0pEOQ7lxhD\nv0O2R5mQhyV5gsoCYypLKS9gA+UuyKuq42POXq0gz9QDyD5CMpep0EZXBuNln3Lf5RPICpaFsvnH\n6P9c1qUhv43rFjGtKRWOVxYkrf7+V+LYRR++56HO+00E/Uvoc3tCQXQpR1xAgTpdHs+vo/+frw/D\nrRB7mJvpkta4irYyvPnJ22PPfOb2uENBCzPyQLXatM2CpNxxjmtmNfojaEFeSYc0n+5hHEwS8koe\nYaxM6nVZqCKP1+pdBJI8TymALwXqveeTnLlPktcFbQ+h3G+dDnmdUg7NK9oekkwg/0wfoDyT51hb\nnRY9r7ZIy6A/2z7myLGFfigon2Q1oi0PNeaBF1KA5xaCrZYFPdcGaLsHh5Dgnp6/e3tsWrjvVyRY\nyy2HguBeYP52e+RxLyIWBS32SJ5dUSDgs0uMjaMu1pf3XNpqkNFWE5I2RxaO/RGu+bCPspoQda4b\naN8Fhqr0Wuvl/jDUMqUoiqIoirIB+jKlKIqiKIqyAXcq81lNBDRk854YmPH6JM+lLnlDkfSyCGB+\nnJTwDIgpQFd/H+bQroVrDp9CJnjwGOcXAtPt7hhmwisq5vSYJCYRkR7JWCQNjksEJdunXEcUS01c\nC+bUkszdMXkrGQdeflEJ0/eQpKS0gtkzXpGURBJGmuH8bVHZ5CXUooCPFPAynX3+9rg+QZs8HKBf\nIwum19UMjd0pYc63KX9ft4s8bWUAuXO5RH2b9PnFgkzv2ZPbw14HY0hEpKIglA8jeHPZ5MUTT6l8\nJFlXlLOu0YMcW1DAx4sFrvPQJWnEhyn5aobPQ/LY9GsMnGVAUts2IWlltcB8XKbkzWihzq0SMkHd\nRfkuKPBqTbJgukAwyCMfY0EE579JsuDSxefu6+ib8RjtOCZvsFWwLmVblBdv+guYt3YED52wTx55\ni7dvj68WkJWTS0jPaYJ6GpqnOyHko4rkPI882poeypOSMn9YbH9uxoJ1IKPh8owCG94rMNY8F1sR\navK6jDzyfMwxp+oe2rBXQ85ZcRDODG3VbGG+Z5T7rmdj/VyuKDfiC/VxKGejF1EwxxgSluOR3Eby\nz3yMMWV8rKeFjzqvzjA+JrSODxd4Vlhv4HybvM/n5GkXkQfbNulSvtqY++cc83S/hefJjLajNCkY\n9XJO2yJ20aZDyos4vmTJD4PHamDdzcnr2iKP1dMZ1kGb8unNXGzBEBFpUiDkgHp7MYWH4RFJr1MP\n/VyeYpx0AtqaQIFqOwWuuXtIWyr20I5WjPVoeY46rGg3wqp+OVuTWqYURVEURVE2QF+mFEVRFEVR\nNuBOZb4+5a+j2Jyya2DGt3dgrrUq8tag/DmWBzNe9xIyUbeFi7bI7FeR51ynDdPlfA6zYlDg87MZ\nzJCGAim2Hqzn/0rIxDnNYeL0KB/SjOSAZIVzSjLFiwWpchjjHsEc0ktKucd+4y5MqJ9boC2C+Oz2\n+J0hzPJlhPtuCxPC3H51hfLfL9HWrTZspjblHJwvyaxqQc7zKRdho4TZemcXbZjXuG9GUV3TDKbn\nZIT2DybwQkkcyEuDnXW5bFKgTasJ7iGUs69HQVtbA9QzpfE7TdHubWoLSq0oJxTEMJyRLNrGdLRI\nvm42WBKGN982qeGoJgOB9LhLMnWYoP7epzHvmiSrlCTDZBnl2mKPTDLhj98jSZE8+BYXuGbHR98G\nfchlF1PMic5qXYIPmgjoedCjfH4UtNeNIF04grVjPIW8M2hg3J67qFt8Rl5ltL48yFCfVowx1qI8\ngsEVzi8anMtwO0wEckbLxdhskAx1VWDuNLoUFDI9uj0uS7TDtEJbs0ib0dg8HaJNDh+SNyV5UM4M\nSXBN2tLRxHhvpetzs6jx/cUcbRdT4NyS6tZhp/EU7bvM37k9diaQnux9XLMkT8jBANLetEf6DwXI\nFFp3ygHmxDZp+5g7iznkKbdLkjLlL/RTm85Hf1aUN7ISuJGWK9RhPobcXQYUFDlHn+zENI5alDfw\nkIKWkod+cL7ushqSvBvXmM+dJsZtSrkDAxqHSw+daweo/2qJa7YDjFUTYf2akbeg6+K5Gd3DXPA8\nzPeAvLQ/CmqZUhRFURRF2QB9mVIURVEURdmAO5X5VnOYwKcJjMXNT1BepRkF8CSPkJJMvfE5zIxZ\nA6bedgPvhv0ezNuGgsxllGstF5SnHpCrHXk3jMg750EIE6CISCeAeXCaoty2i+/3EnxuXcHkuGpC\nurqk/ILelDyg2jB1egJz6ImN+rgBJDy/A9P1HuW8S6+2/85cnKLPGtTWxWOY7l+jQJXDJdrk6gTH\n3Q7q3iavQ5sDp55BAvAz3GthIIOWFCyy20Q71DVMvg6Nj2K57v3VyWGKHmUk4dnop9JHnc9PIEmd\nxDCHNyvKZ/UWeX89I89Pkp38PpWb3JgSB//YyeB5s2y9mt8/gYs5VczQ3iVJLqsmxm97TLn5PgXz\n/v4lpJ5ZTm1P6RXzHO1SG4yRqEUC0pQkPw9jvDWFxGKT/DdarfuAdVkmiNHGWYl5lJxD9sjpd6VD\n8sHEwvoSpRi38wrndJe45jEFmHywgiRjUXDWuk1lHW8/1+I4gWbbG2HsXFEeuRaVJ9yh4I8RBRA+\nRZtMQvTHjB4brlDuUZKCykvKp+dSAE/BnPVckhdJXjmmvhMRkRHlcmuQlObQeezxWlGbZhRcl/JM\nFnuUGzbFuKt2sS6M+xSkd4j5OyOvVreBsV/L9oMji4gEexR4s0J7JxPaFiAY456HNctqY1uAW6Lc\nhjx2yxnmfkLe9KP3KegsBemderiXS9tV+hX60ydP/GBnfc1aUk69qYd23aU51eljno8mvP2D8vtG\nGDM7Eco0X6BdilOMncMuxmdwhDJ96ghbAsbkkNlIXm5uqmVKURRFURRlA/RlSlEURVEUZQPuVuYj\n7wDbhvnN5A9ujysKbndAwf3iJcx7FXl9HZDnnE256RYkK+xRDqPKxnUuAlS/R1KbUEDHvQjXtKyn\na/WJOVCcIdnDJs9ABxLFYh/nxBZMzlGOcqwynD9/F6b1+5+C+T0ms7dTwlsjtCGxuA7MrGH1Yhi8\nzdnfR5vOyJvLp/xqNuWy61EOr9yG12E8oVxggmuG5DEiJBfNyQ3UM5BgDLW5YygfF7pPxhUFFyU5\nWUQkbMNM3KHAjlUXQThdByb2JUlvOyTJZeS11ZqTrLiH/gtJ9pAJrjOh4Jf5FH186bCX3/aDPIqI\nVCUFUwzQrqsE/TabQxZLBOVOfhFtWVMwyJmgnjlJZ7aLOdioMFG9S1yz6KDPDyhoX1Dg/OOccpBd\nrHvFeaQGxRNIGm36Q+LiWt0Qa83sOXndziGTNAaQFV7PIQ0sm7j+I1p4CupDIUlisSLPK5u8erfE\ncow+OPbgsWw7mCOXVHdSsqXqojxxF/0xJ+m7SZKq79FY2UeA04IlnxLj2i3JM62H+dGhtop664+l\nhNbpSsgb0yZZuER9ZoYCpPbgkUeKpJgAlR6S5Nm4oCDIs19ze5xTPsklBR6d22ij+uIFeXJLHD2A\nJ+F0QYFtC4yj9Bjza0GBnKMa86JwsR67Fo4D8kz2Wlj72j0KTkqBQNMJzh8do287RyhP2cCcsF5I\nWdgmmdCP6LxjlPX8lJ6bOZ4FhwdYR5sdfDc/xRjJfIzhtx5SvtrXKSegg3Hbae3PuxAAAAoOSURB\nVKMtmh557Bcv99xUy5SiKIqiKMoG6MuUoiiKoijKBtypzBfaZPanQHeJTcE5M5j06wFMtxzozqHv\n1uRJ1hB8vpyx9xtMtGkBs70VwwSYR/Bc8CzIju2Cr7lukp83yFOCTKL2lAKL4evSImmksUL5hKSh\nFQUxC22YGZeUF+yNDsy+I5KuFmcUKC9FnQMhT8UtEZPcVlEgvuEQ5UkMTPdBH20SdSCPNmN8nlEe\nuPkKbRWWaAeXArJF5DwzRHXldAgTcbGCBNXfw3XSvfWAbCWNqa98AEkmK/H98w7lPnwO0/C0hPwl\nFfrvvIRJ+rBHec7IE87tkh7dQ5ue2TBDjycYZ84xnb9FQjLFx+QZVCUw+88WKJ/VgVTnTNAuZ5eY\nR/U9yEo5zWt7H2MEYpnIM5KhCvKcy0ckSbUgsQxI8my21uXPpg85rxhAnjfPkc/PsjjHGu7nUo5L\nh+SQVoFxvqCcbE4J6WVMXrRt8v5MSSId2GjfaG/73ny7DzF2vOXh7bEdol4T8uyraf7a5GnlF6hX\nTFsRkh7m4M6IcjEGGCvu4N7tcYskwqGDeRCs0C/jHfRF4a7/xq9KCgqb4vsRjdMZBed0KTfqnJ4J\n+Q7pfE8xpvIGzWXy/o1ctFdi4XO3wMJTUGDauOKtA5+SbeFTbsJJhP4hx1lZuVh34jHpth6CqkY1\nxmPZRd8eJRjLFj2jmyHJpeydSVsC3H3ytKPneJOCYr5/tb5mZRGkZ+8YY+ZqgjVlQs8v16WtPA76\n01+hn+sQ685DyhGYUtDl5RT9VpIn/0UX4623xDjvdylQ60dALVOKoiiKoigboC9TiqIoiqIoG3Cn\nMt/cgWdFkEK6iCcwy1kJTJTOBczVQZO80zoodkgyXEJByYak+8xymC53ST4RwX3zFNdsh+QZVFDw\nS2fdY8ilvGpXGb5/3yevgQAmyl6N84ckmVxdUBA0j+RMQ7IoSQ8/T24pbfJci32YLmMKXDfx8Pm2\ncPoU2HSJMoQUnTFvoQx1QaZnF3JOtqJzSHqwbfKEs6idlzA338shxzX6JP2GkHjm5Slua1Hex2Jd\nXml0UL6n76N9u+QVZiyUe7hAYMSwA1N1WKDvGyF5IQVoryEFK9yhgJSNJaSO1xK047tzMpOTN+o2\nCcl5UsiJxW7A83KnhmwbzjGXFx4krFGGPuyfo9/CFuUaHEN6SErK7ZVBJihofizJE7DiwK4lpMPM\nWc8/WeckAc4xj6Yp+nanBVkmvcS9TYBxtaJ8Zum7GHuHe5iPS5L/zq7QPx0H1z98hLK6JHnX+fZz\n84UZPFCdQ8oDGGKMt84hkYxzrJV9G+vyPCHPWZJLzBiBN99rkfw3wr0GPmSXFa2HVYE2mYeQr+wY\n/VW76zlQuzvkMUgyYWOG43GE8eVluIezwlxbCOaX0yMJj2SrNq1ZKQVp9qkOcUVrDW2tkOb2vaZF\nREbvwUsyobUpexd1zqgfuPWynLw5A3h+ZwZbEHLKaVuWtH2DJLj9Nnna7pP3LuXJjTy0Y6PGM21w\nnxLxioi8hzm/iNCWNm1rOdzHwluRx6sl5Dke0PikPsnIK3oV0/VL5B0MBNe/eE5bYigP4lXycrYm\ntUwpiqIoiqJsgL5MKYqiKIqibMCdynyPOzDjhfabt8crMhtnO5A0JpS366iA2diiXfn1fZgfzYxc\nktoUEI5y+wQHMF0uyeRsVzCOTiiQ5CWZT+tn614JNQXtnJK5vtVEOXbJFHkyJVMp5TnLyRvMIpOz\n0yC5sAHPglkKN47TGm00cFGeZYjr77jrASq3QTEm03iO6/d8mNKFPOTiKUz1BeVzmjuo+/0I0sPp\nOXlElji/JI+UIXlmio37Zi0K8la9fnv8C6eQ5jrOumdmlMO8m/swn//iFOXrGMpBSDkCjcG9Uwro\n18lgzp59Dm1hKHfjlIZUSpKahPhucx/SUbiCqX6b5BYFs6XAgH6G+gSUH9NNIasZC5XYa+LzgGSl\ngGSiRPC5G6Dd+yvMoeqIAtDOMXZsCha7b0HyGL3gaVs8Qzko/aFULXgo9SkY6kmA8tU26vk6BXQU\nyqnnUTBElzxzY/8dnO9Dznw2QVnvk9di6PG2g+0wJ+9le4p67dFqP6Rotm3K9ZnUmHcVSdZljPat\nKbiuu8S4nlCA10vyKLN2UIbA4JptH2uxd4+k1cW6ZDum3IfeAOPlwsX9zjy06f4Cc/ZEIN/2RpAS\nLzrwcgxoDbp0MVjqiuQlyifZaELaC7t0fp/Gyha5nOMezV1a14W8n5do4ybJjX5AUnaF9n5GsvZ8\nTLLdAnVOD97HMUnlgwJ9FVPO2DzDOf7rtK67668Z40eYX70W2tXZI0l2Ru7ZtOYHHq6VzvDeEFBO\nTM592aaAyoc25uPM4F6eQX2W5JHp2S8nwatlSlEURVEUZQP0ZUpRFEVRFGUD7lTmS8ibwiPz9r0A\nJuerBgVWO6eAWzXMbxW5G+VPIMn0MwqeGOFeVU3VnEPCyGZ4l3xO0tB8CZNhlZAs0Fw3yb+/IkmD\ngmdePkFZZ33yOKtQvpjyBdoh1Yckg8zApBuRR9OigKnbZDB1TimHUduH+bSgXHPbYu7DxN7J0Ger\nOeRYvyZZMydPFx+fO+RpOLxA+3iCtp5G6Kd0jP6uhHM8oc/8Eu12MUG/dHfeuj2O559Zq8+MJINm\nCxJAK6acbSt41Qya6INVm7wEKejqnGSxbgf9nZ1S8EeKNRlSvjSHpJeejSCyWXv7AVhFRO69BRN4\n94xM/SvKeUj5EnMPBe+SR86DCF4y2Qx1EAvjtGyxxyO+e0XB8+wV2rTVJO8pyvHYJLllzgnmROTR\nPYy3GeUV+zLhvqItAiTnFXN4gNZNhBVt7KDtowXKmpBX8M4A7WjOKGCsQ55HCcrmHJC76JY4v8Ia\n1/VRtvfJOy1wMU8nY3h8JVPybPIoD9wp6n7YJtdPWhJLkpdyB33TyzEfHQdy8tkQ7dMgL9DFbH2M\nXzmQpNxTlLUiv7XVFGvQ++TZJYJ7XE6xFsyGKAfFAZVuA8+NOkblEvKUDmlrxbiL60fPty/ZiogM\n/t/27mClYTCIwmgSmoaitijoMxR8/xfqWhBXoQWNyxlE6OJuz1mHNrRp8pEw/d/qvHC7tCnkpT7j\np7am4tgmFac22biudezP+7aWYbsmHtr0+e2lvudpqvd6Het68n2t4+hx19b+G9ufs859VHgYPo/1\n/fy09XSfj/W627lNLfYJ9486Tr7a73Sda/9ObS3I09LOu0vtx3t7nHt9qO337bHt8GcI8R53pgAA\nAmIKACAwbtt2fysAAP7lzhQAQEBMAQAExBQAQEBMAQAExBQAQEBMAQAExBQAQEBMAQAExBQAQEBM\nAQAExBQAQEBMAQAExBQAQEBMAQAExBQAQEBMAQAExBQAQEBMAQAExBQAQEBMAQAExBQAQEBMAQAE\nxBQAQOAXwxPw+QG21P4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158a89690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
